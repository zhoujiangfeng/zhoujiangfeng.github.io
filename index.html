<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>都灵的夏天</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="都灵的夏天">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="都灵的夏天">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="周江峰">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="都灵的夏天" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 6.2.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">都灵的夏天</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="搜索"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-hello-world" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2022/06/21/hello-world/" class="article-date">
  <time datetime="2022-06-21T13:41:32.111Z" itemprop="datePublished">2022-06-21</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2022/06/21/hello-world/">Hello World</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Welcome to <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a target="_blank" rel="noopener" href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a target="_blank" rel="noopener" href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a target="_blank" rel="noopener" href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="quick-start">Quick Start</h2>
<h3 id="create-a-new-post">Create a new post</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="run-server">Run server</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="generate-static-files">Generate static files</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="deploy-to-remote-sites">Deploy to remote sites</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2022/06/21/hello-world/" data-id="cl4od7abc000b7fhobl3w8qtk" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-2022-04-12-SeqGAN" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2022/04/12/2022-04-12-SeqGAN/" class="article-date">
  <time datetime="2022-04-12T08:45:27.104Z" itemprop="datePublished">2022-04-12</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="seqgan具有policy梯度的序列生成对抗网络">SeqGAN：具有Policy梯度的序列生成对抗网络</h1>
<h2 id="publish">Publish</h2>
<p>AAAI-2017 ## title SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient ## solution problem 生成对抗网络（GAN）在生成real_value数据取得了巨大的成功,但是在用于生成离散数据具有局限性，主要原因在于：来自生成模型的离散输出使其难以从判别模型的梯度更新传递给生成模型。此外：判别模型只能评估完整的序列，而对于部分生成的序列，一旦生成完整序列，就需要去平衡当前和未来的评分。 ## Summary 1. 提出一种训练生成模型的新方法SeqGAN<br />
2. 其数据生成器建模使用强化学习中的随机策略，其中RL奖励值来自GAN判别器对完整序列的评判。使用蒙特卡洛搜索传回中间状态。 ## Conclusion 在合成数据和现实任务上进行的大量实验表明，与强大的基线相比，有了显著的改进. # Other ## 蒙特卡洛搜索 学习资料 <a target="_blank" rel="noopener" href="https://blog.csdn.net/luomin2523/article/details/118109154">1</a> <a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_16137569/article/details/83543641">2</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2022/04/12/2022-04-12-SeqGAN/" data-id="cl4od7ab900047fhodc0b14fs" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-2022-05-26-「数据增强」2022NAACL：TextSmoth" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2022/04/12/2022-05-26-%E3%80%8C%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E3%80%8D2022NAACL%EF%BC%9ATextSmoth/" class="article-date">
  <time datetime="2022-04-12T08:45:27.104Z" itemprop="datePublished">2022-04-12</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="text-smoothing一种数据结合mix-up的数据增强方法">Text Smoothing：一种数据结合mix-up的数据增强方法</h1>
<h2 id="publish">Publish</h2>
<p>2022ACL</p>
<h2 id="title">title</h2>
<p>Text Smoothing: Enhance Various Data Augmentation Methods on Text Classification Tasks ## solution problem 进入神经网络之前，标记通常被转换为对应的One-hot表示，这是词汇表的离散分布。平滑表示是从预先训练的MLM中获得候选token的概率，它可以被视为对onr-hot表示的更多信息的替代。我们提出了一种有效的数据增强方法，称为文本平滑，通过将句子从其one-hot表示转换为可控的平滑表示。我们在低资源条件下对不同基准的文本平滑进行了评估。实验结果表明，文本平滑方法的性能明显优于各种主流数据增强方法。此外，文本平滑可以与这些数据增强方法相结合，以获得更好的性能。</p>
<p><img src="https://cdn.jsdelivr.net/gh/zhoujiangfeng/images@main/20220526171229.png" alt="image-20220526171221247" style="zoom:50%;" /></p>
<p><img src="https://cdn.jsdelivr.net/gh/zhoujiangfeng/images@main/20220526175945.png" alt="image-20220526175945737" style="zoom:50%;" /></p>
<p>文本平滑代码：Pytorch</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sentence = <span class="string">&quot;My favorite fruit is pear .&quot;</span></span><br><span class="line">lambd = <span class="number">0.1</span> <span class="comment"># interpolation hyperparameter</span></span><br><span class="line">mlm.train() <span class="comment"># enable dropout, dynamically mask</span></span><br><span class="line">tensor_input = tokenizer(sentence, return_tensors=<span class="string">&quot;pt&quot;</span>)</span><br><span class="line">onehot_repr = convert_to_onehot(**tensor_input)</span><br><span class="line">smoothed_repr = softmax(mlm(**tensor_input).logits[<span class="number">0</span>])</span><br><span class="line">interpolated_repr = lambd * onehot_repr + (<span class="number">1</span> - lambd) * smoothed_repr</span><br></pre></td></tr></table></figure>
<ol type="1">
<li><p>使用BERT作为MLM，给定下游数据集命名为：<span class="math inline">\(D=\{t_i,p_i,s_i,l_i\}_{i=1}^{N}\)</span> ,N表示样本数量，<span class="math inline">\(t_i\)</span>表示文本one-hot 编码，<span class="math inline">\(p_i\)</span>表示<span class="math inline">\(t_i\)</span>位置编码，<span class="math inline">\(s_i\)</span>表示<span class="math inline">\(t_i\)</span>的段编码，<span class="math inline">\(l_i\)</span>表示实例标签。</p></li>
<li><p>将<span class="math inline">\(t_i,p_i,s_i\)</span>送入BERT</p></li>
<li><p>取回BERT中Transformer-encoder最后一层的输出表示为 <span class="math display">\[
\overrightarrow{t_i}=BERT(t_i)
\]</span> 其中<span class="math inline">\(\overrightarrow{t_i}\)</span>形状为[seq_len,emb_size]</p></li>
<li><p>然后乘以<span class="math inline">\(\overrightarrow{t_i}\)</span>乘以BERT中词嵌入矩阵<span class="math inline">\(W\)</span>,其形状为[vocab_size,embed_size] <span class="math display">\[
MLM(t_i)=softmax(\overrightarrow(t_i)W^T)
\]</span> 其中<span class="math inline">\(MLM(t_i)\)</span>中每一行是token词汇表中的概率分布，表示了预训练BERT学习到输入文本所在位置的包含上下文 的标记选项（信息）。</p></li>
<li></li>
<li><p>mixup定义为 <span class="math display">\[
\tilde{x}=\lambda x_{i}+(1-\lambda) x_{j}
\]</span></p>
<p><span class="math display">\[
\tilde{y}=\lambda y_{i}+(1-\lambda) y_{j}
\]</span></p>
<p>其中<span class="math inline">\((x_i,x_j),(y_i,y_j)\)</span>为从训练数据中随机抽出两个目标特征向量,<span class="math inline">\(\lambda\in[0,1]\)</span>在文本平滑中，One-hot表示和平滑表示来自相同的原始输入，标签相同，其内部插入操作不会改变标签，因此mixup操作可以简化为 <span class="math display">\[
\widetilde{t_{i}}=\lambda \cdot t_{i}+(1-\lambda) \cdot \operatorname{MLM}\left(t_{i}\right)
\]</span> 其中<span class="math inline">\(t_i\)</span>为one-hot表示，<span class="math inline">\(MLM(t_i)\)</span>为平滑表示，<span class="math inline">\(\widetilde{t_i}\)</span>为联合插入表示，<span class="math inline">\(\lambda\)</span>为用于控制插入的超参数。下游任务中我们使用联合表示代替one-hot变化表示作为输入。</p></li>
</ol>
<h2 id="experiment">Experiment</h2>
<ul>
<li><p>数据集</p>
<p><img src="https://cdn.jsdelivr.net/gh/zhoujiangfeng/images@main/20220526201719.png" alt="image-20220526201719743" style="zoom:50%;" /></p></li>
<li><p>Baseline</p>
<table>
<colgroup>
<col style="width: 27%" />
<col style="width: 72%" />
</colgroup>
<thead>
<tr class="header">
<th>approaches</th>
<th>简介</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>EDA</td>
<td>由四个简单操作组成：同义词替换、随机插入、随机交换和随机删除。</td>
</tr>
<tr class="even">
<td>Back Translation</td>
<td>将句子翻译成临时语言(EN-DE)，然后将先前翻译的文本翻译回源语言(DE-EN)</td>
</tr>
<tr class="odd">
<td>CBERT</td>
<td>用预先训练的BERT mask一些标记并预测它们的上下文替换。</td>
</tr>
<tr class="even">
<td>BERTexpand, BERTprepend</td>
<td>通过在给定类的所有示例中添加类标签来满足BERT条件。“expand”标签以 模拟 词汇表，而“prepend”则没有</td>
</tr>
<tr class="odd">
<td>GPT2context</td>
<td>为预先训练的GPT模型提供提示，并持续生成，直到[EOS]token</td>
</tr>
<tr class="even">
<td>BARTword, BARTspan</td>
<td>通过在给定类的所有示例前添加类标签来为条件BART。BARTword屏蔽了单个单词，而BARTspan屏蔽了连续的区块。</td>
</tr>
</tbody>
</table></li>
<li><p>结果</p>
<figure>
<img src="https://cdn.jsdelivr.net/gh/zhoujiangfeng/images@main/20220526202740.png" alt="image-20220526202740487" /><figcaption aria-hidden="true">image-20220526202740487</figcaption>
</figure></li>
</ul>
<h2 id="conclusion">Conclusion</h2>
<p>小数据，可控，目前优于其他，未来，结合其他DA=顶会。我也想发顶会啊。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2022/04/12/2022-05-26-%E3%80%8C%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E3%80%8D2022NAACL%EF%BC%9ATextSmoth/" data-id="cl4od7aba00057fhoaa2yf9mu" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-2022-05-27-「GAN」2021EMNLP：对LGAN进行Counter-Contrastive学习" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2022/04/12/2022-05-27-%E3%80%8CGAN%E3%80%8D2021EMNLP%EF%BC%9A%E5%AF%B9LGAN%E8%BF%9B%E8%A1%8CCounter-Contrastive%E5%AD%A6%E4%B9%A0/" class="article-date">
  <time datetime="2022-04-12T08:45:27.104Z" itemprop="datePublished">2022-04-12</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="counter-contrastive-学习训练language-gan">Counter-Contrastive 学习：训练Language GAN</h1>
<h2 id="publish">Publish</h2>
<p>2021EMNLP</p>
<h2 id="title">title</h2>
<p><a target="_blank" rel="noopener" href="https://aclanthology.org/2021.findings-emnlp.415.pdf">Counter-Contrastive Learning for Language GANs</a></p>
<h2 id="摘要">摘要</h2>
<p>生成对抗网络(GANs)在图像合成方面取得了巨大的成功，但在生成自然语言方面存在一定的困难。挑战来自于鉴别器传递的uninformative learning signal。换句话说，<strong>糟糕的learning singnal限制了生成结构丰富、语义丰富的语言的学习能力</strong>。在本文中，我们提出在语言gan中采用反对比学习(CCL)方法来支持生成器的训练。与标准的GANs采用简单的二元分类器来区分样本的真假相比，我们采用了一种反对比学习信号，通过提高语言合成器的训练</p>
<ul>
<li>(1)把生成样本与真实样本 放在一起（以生成真实的数据）</li>
<li>(2)推开真实的样本（以阻碍鉴别的训练）从而防止鉴别器被过度训练。</li>
</ul>
<p>我们在合成基准和实际基准上评估了我们的方法，并与以前的GAN相比，在对抗序列生成方面产生了具有竞争力的性能。</p>
<h2 id="solution-problem">Solution problem</h2>
<p>CCL <span class="math display">\[
\mathcal{L}_{i}=-\log \frac{e^{\operatorname{sim}\left(\mathbf{h}_{i}, \mathbf{h}_{i}^{-}\right) / \tau}}{\sum_{j=1}^{N}\left(e^{\operatorname{sim}\left(\mathbf{h}_{j}, \mathbf{h}_{j}^{-}\right) / \tau}+e^{\operatorname{sim}\left(\mathbf{h}_{j}, \mathbf{h}_{j}^{+}\right) / \tau}\right)}
\]</span></p>
<h2 id="experiment">Experiment</h2>
<h2 id="conclusion">Conclusion</h2>
<p>没代码，不看了。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2022/04/12/2022-05-27-%E3%80%8CGAN%E3%80%8D2021EMNLP%EF%BC%9A%E5%AF%B9LGAN%E8%BF%9B%E8%A1%8CCounter-Contrastive%E5%AD%A6%E4%B9%A0/" data-id="cl4od7aba00067fho9nf7gz05" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-2022-05-28-「数据增强」2022ACL：Glitter" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2022/04/12/2022-05-28-%E3%80%8C%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E3%80%8D2022ACL%EF%BC%9AGlitter/" class="article-date">
  <time datetime="2022-04-12T08:45:27.104Z" itemprop="datePublished">2022-04-12</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="text-smoothing一种数据结合mix-up的数据增强方法">Text Smoothing：一种数据结合mix-up的数据增强方法</h1>
<h2 id="publish">Publish</h2>
<p>2022ACL</p>
<h2 id="title">title</h2>
<h2 id="solution-problem">Solution problem</h2>
<p>进入神经网络之前，标记通常被转换为对应的One-hot表示，这是词汇表的离散分布。平滑表示是从预先训练的MLM中获得候选token的概率，它可以被视为对onr-hot表示的更多信息的替代。我们提出了一种有效的数据增强方法，称为文本平滑，通过将句子从其one-hot表示转换为可控的平滑表示。我们在低资源条件下对不同基准的文本平滑进行了评估。实验结果表明，文本平滑方法的性能明显优于各种主流数据增强方法。此外，文本平滑可以与这些数据增强方法相结合，以获得更好的性能。</p>
<p><img src="https://cdn.jsdelivr.net/gh/zhoujiangfeng/images@main/20220526171229.png" alt="image-20220526171221247" style="zoom:50%;" /></p>
<p><img src="https://cdn.jsdelivr.net/gh/zhoujiangfeng/images@main/20220526175945.png" alt="image-20220526175945737" style="zoom:50%;" /></p>
<p>文本平滑代码：Pytorch</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sentence = <span class="string">&quot;My favorite fruit is pear .&quot;</span></span><br><span class="line">lambd = <span class="number">0.1</span> <span class="comment"># interpolation hyperparameter</span></span><br><span class="line">mlm.train() <span class="comment"># enable dropout, dynamically mask</span></span><br><span class="line">tensor_input = tokenizer(sentence, return_tensors=<span class="string">&quot;pt&quot;</span>)</span><br><span class="line">onehot_repr = convert_to_onehot(**tensor_input)</span><br><span class="line">smoothed_repr = softmax(mlm(**tensor_input).logits[<span class="number">0</span>])</span><br><span class="line">interpolated_repr = lambd * onehot_repr + (<span class="number">1</span> - lambd) * smoothed_repr</span><br></pre></td></tr></table></figure>
<ol type="1">
<li><p>使用BERT作为MLM，给定下游数据集命名为：<span class="math inline">\(D=\{t_i,p_i,s_i,l_i\}_{i=1}^{N}\)</span> ,N表示样本数量，<span class="math inline">\(t_i\)</span>表示文本one-hot 编码，<span class="math inline">\(p_i\)</span>表示<span class="math inline">\(t_i\)</span>位置编码，<span class="math inline">\(s_i\)</span>表示<span class="math inline">\(t_i\)</span>的段编码，<span class="math inline">\(l_i\)</span>表示实例标签。</p></li>
<li><p>将<span class="math inline">\(t_i,p_i,s_i\)</span>送入BERT</p></li>
<li><p>取回BERT中Transformer-encoder最后一层的输出表示为 <span class="math display">\[
\overrightarrow{t_i}=BERT(t_i)
\]</span> 其中<span class="math inline">\(\overrightarrow{t_i}\)</span>形状为[seq_len,emb_size]</p></li>
<li><p>然后乘以<span class="math inline">\(\overrightarrow{t_i}\)</span>乘以BERT中词嵌入矩阵<span class="math inline">\(W\)</span>,其形状为[vocab_size,embed_size] <span class="math display">\[
MLM(t_i)=softmax(\overrightarrow(t_i)W^T)
\]</span> 其中<span class="math inline">\(MLM(t_i)\)</span>中每一行是token词汇表中的概率分布，表示了预训练BERT学习到输入文本所在位置的包含上下文 的标记选项（信息）。</p></li>
<li></li>
<li><p>mixup定义为 <span class="math display">\[
\tilde{x}=\lambda x_{i}+(1-\lambda) x_{j}
\]</span></p>
<p><span class="math display">\[
\tilde{y}=\lambda y_{i}+(1-\lambda) y_{j}
\]</span></p>
<p>其中<span class="math inline">\((x_i,x_j),(y_i,y_j)\)</span>为从训练数据中随机抽出两个目标特征向量,<span class="math inline">\(\lambda\in[0,1]\)</span>在文本平滑中，One-hot表示和平滑表示来自相同的原始输入，标签相同，其内部插入操作不会改变标签，因此mixup操作可以简化为 <span class="math display">\[
\widetilde{t_{i}}=\lambda \cdot t_{i}+(1-\lambda) \cdot \operatorname{MLM}\left(t_{i}\right)
\]</span> 其中<span class="math inline">\(t_i\)</span>为one-hot表示，<span class="math inline">\(MLM(t_i)\)</span>为平滑表示，<span class="math inline">\(\widetilde{t_i}\)</span>为联合插入表示，<span class="math inline">\(\lambda\)</span>为用于控制插入的超参数。下游任务中我们使用联合表示代替one-hot变化表示作为输入。</p></li>
</ol>
<h2 id="experiment">Experiment</h2>
<ul>
<li><p>数据集</p>
<p><img src="https://cdn.jsdelivr.net/gh/zhoujiangfeng/images@main/20220526201719.png" alt="image-20220526201719743" style="zoom:50%;" /></p></li>
<li><p>Baseline</p>
<table>
<colgroup>
<col style="width: 27%" />
<col style="width: 72%" />
</colgroup>
<thead>
<tr class="header">
<th>approaches</th>
<th>简介</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>EDA</td>
<td>由四个简单操作组成：同义词替换、随机插入、随机交换和随机删除。</td>
</tr>
<tr class="even">
<td>Back Translation</td>
<td>将句子翻译成临时语言(EN-DE)，然后将先前翻译的文本翻译回源语言(DE-EN)</td>
</tr>
<tr class="odd">
<td>CBERT</td>
<td>用预先训练的BERT mask一些标记并预测它们的上下文替换。</td>
</tr>
<tr class="even">
<td>BERTexpand, BERTprepend</td>
<td>通过在给定类的所有示例中添加类标签来满足BERT条件。“expand”标签以 模拟 词汇表，而“prepend”则没有</td>
</tr>
<tr class="odd">
<td>GPT2context</td>
<td>为预先训练的GPT模型提供提示，并持续生成，直到[EOS]token</td>
</tr>
<tr class="even">
<td>BARTword, BARTspan</td>
<td>通过在给定类的所有示例前添加类标签来为条件BART。BARTword屏蔽了单个单词，而BARTspan屏蔽了连续的区块。</td>
</tr>
</tbody>
</table></li>
<li><p>结果</p>
<figure>
<img src="https://cdn.jsdelivr.net/gh/zhoujiangfeng/images@main/20220526202740.png" alt="image-20220526202740487" /><figcaption aria-hidden="true">image-20220526202740487</figcaption>
</figure></li>
</ul>
<h2 id="conclusion">Conclusion</h2>
<p>小数据，可控，优于其他，未来，结合其他DA=顶会。我也想发顶会啊。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2022/04/12/2022-05-28-%E3%80%8C%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E3%80%8D2022ACL%EF%BC%9AGlitter/" data-id="cl4od7abb00087fho2dsv2dse" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-2022-06-14-「数据增强」2022NAAC：TreeMix" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2022/04/12/2022-06-14-%E3%80%8C%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E3%80%8D2022NAAC%EF%BC%9ATreeMix/" class="article-date">
  <time datetime="2022-04-12T08:45:27.104Z" itemprop="datePublished">2022-04-12</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="treemix基于组合的数据增强方法">TreeMix：基于组合的数据增强方法</h1>
<h2 id="publish">Publish</h2>
<p>NAACL2022</p>
<h2 id="title">Title</h2>
<p>TreeMix: Compositional Constituency-based Data Augmentation for Natural Language Understanding</p>
<h2 id="abstract">Abstract</h2>
<p>数据增强是解决过度拟合问题的一种有效方法。前人针对自然语言处理提出了不同的数据扩充策略，如噪声注入、单词替换、回译等。虽然有效，但它们<strong>忽略了语言的一个重要特征-组合性</strong>，<strong>复杂表达的意义是从其子部分建立起来的</strong>。受此启发，我们提出了一种用于自然语言理解的成分数据扩充方法TreeMix。具体地说，<strong>TreeMix利用选区分析树将句子分解成构成子结构，并利用Mixup数据增强技术对它们进行重组以生成新的句子</strong>。与以前的方法相比，TreeMix在生成的样本中引入了更大的<strong>多样性</strong>，并鼓励模型学习NLP数据的<strong>组合性</strong>。在文本分类和SCAN的大量实验表明，TreeMix的性能优于目前最先进的数据增强方法。</p>
<h2 id="solution-problem">Solution problem</h2>
<p>合成性是语言的一个关键方面，因为复句的意义是从它的子部分建立起来的。先前的工作还表明，语法树(例如，基于树的LSTM)有助于对句子结构进行建模，以便更好地进行文本分类。然而，在语言技术社区中，除了在语义分析方面的一些例外情况外，利用组合结构进行数据扩充并没有受到太多关注</p>
<figure>
<img src="https://cdn.jsdelivr.net/gh/zhoujiangfeng/images@main/20220613135323.png" alt="image-20220613135315056" /><figcaption aria-hidden="true">image-20220613135315056</figcaption>
</figure>
<p>我们提出了一种用于自然语言理解的成分数据增强方法，即TreeMix(图1)。TreeMix是一种输入级混合方法，它利用成分分析信息，将来自不同句子的不同片段(子树的短语)进行重组，以创建训练集中从未见过的新示例；同时还将基于这些片段策略性地创建新的软标签。这样，TreeMix不仅利用了构成语言的特征来增加扩充的多样性，而且为这些混合的例子提供了合理的软标签。</p>
<p>mixup定义为 <span class="math display">\[
\tilde{x}=\lambda x_{i}+(1-\lambda) x_{j}
\]</span></p>
<p><span class="math display">\[
\tilde{y}=\lambda y_{i}+(1-\lambda) y_{j}
\]</span></p>
<p>其中<span class="math inline">\((x_i,x_j),(y_i,y_j)\)</span>为从训练数据中随机抽出两个目标特征向量,<span class="math inline">\(\lambda\in[0,1]\)</span></p>
<p>我们通过融入语言的组合性来改进Mixup，这是泛化所必需的一个关键特征，但神经模型往往无法捕捉到这一点。我们新提出的方法TreeMix不是用整个样本进行内插，而是<strong>通过删除句子的短语并重新插入其他句子的子部分来创建新句子</strong>。TreeMix利用选民树将句子分解成有意义的组成部分，然后将这些组成部分移除并重新组合，以生成新的扩充样本。我们的目标是通过对TreeMix生成的大量样本进行训练来提高模型的组合性泛化能力。一个使用TreeMix进行单句分类的例子如上图所示。</p>
<h3 id="treemix-详细过程">TreeMix 详细过程</h3>
<p><span class="math inline">\({x}_{i}=\left\{x_{i}^{1}, x_{i}^{2}, \ldots, x_{i}^{l}\right\}\)</span>表示长度为<span class="math inline">\(l\)</span>的序列，其对应的one-hot编码label为<span class="math inline">\(y_i\)</span>,我们在<span class="math inline">\(x_i\)</span>上运行一个解析器得到它的解析树<span class="math inline">\(T(x_i)\)</span>,为了获取序列中有意义的子部分，采用递归遍历解析树，获得所有具有一个以上child的子树。表示子树的集合为<span class="math inline">\(S(x_i)= \{t_i^k\}\)</span>.其中<span class="math inline">\(t_i^k\)</span>表示样本<span class="math inline">\(x_i\)</span>的第k个子树，对于子树<span class="math inline">\(t_i^k\)</span>连续覆盖了<span class="math inline">\(x_i\)</span>的<span class="math inline">\(t_{i}^{k} \triangleq\left[x_{i}^{r_{k}}, \ldots, x_{i}^{s_{k}}\right]\)</span>,索引<span class="math inline">\(r_k\)</span>为开始，<span class="math inline">\(s_k\)</span>为结束。例如图一左侧所示，例句子树可以cover span 的有1.<em>this poor film</em>,2. <em>in this poor film</em>, 3.<em>no interest ...etc</em></p>
<p><img src="https://cdn.jsdelivr.net/gh/zhoujiangfeng/images@main/20220614152117.png" style="zoom:50%;" /></p>
<p>对于给定的样本<span class="math inline">\((x_i,y_i)\)</span>，我们从训练集中随机抽取另一个数据点<span class="math inline">\((x_j, y_j)\)</span>。我们对这两个句子运行选区解析器，得到它们的子树集<span class="math inline">\(S(x_i)\)</span>和<span class="math inline">\(S(x_j)\)</span>，我们可以对要交换的子树进行采样。我们引入两个额外的超参数<span class="math inline">\(λ_L\)</span>和<span class="math inline">\(λ_U\)</span>来约束待采样子树的长度。<span class="math inline">\(λ_L\)</span>和<span class="math inline">\(λ_U\)</span>，用子树与原始句子的长度之比来衡量要采样子树的上下限。直观地说，<span class="math inline">\(λ\)</span>控制着我们想要交换的短语的粒度。我们希望交换的长度是合理的。如果它太短，那么交换不能给增强样本引入足够的多样性;否则，如果太长，这个过程可能会给原句注入太多噪音。我们设置λ为比率，以便与原句子的长度不变。表2显示了一些具有不同长度约束的子树示例。我们将长度受限子树集合定义为:</p>
<p><span class="math inline">\(S_{\lambda}(\mathbf{x}) \triangleq\left\{t \mid t \in S(\mathbf{x})\right., s.t. \left.\frac{|t|}{|\mathbf{x}|} \in\left[\lambda_{L}, \lambda_{U}\right]\right\}\)</span></p>
<p>其中<span class="math inline">\(|·|\)</span>表示序列或子树的长度，对于两个句子<span class="math inline">\(x_i\)</span>和<span class="math inline">\(x_j\)</span>,我们随机采样两个子树<span class="math inline">\(t_{i}^{k} \in S_{\lambda}\left(\mathbf{x}_{i}\right)\)</span>和<span class="math inline">\(t_{j}^{l} \in S_{\lambda}\left(\mathbf{x}_{j}\right)\)</span>并且通过<span class="math inline">\(t_j^l\)</span>替换<span class="math inline">\(t_i^k\)</span>构建新的样本。例如 <span class="math display">\[
\overline{\mathbf{x}} \triangleq[x_{i}^{1}, \ldots, x_{i}^{r_{k}-1}, \underbrace{x_{j}^{r_{l}}, \ldots, x_{j}^{s_{l}}}_{t_{j}^{l}}, x_{i}^{s_{k}+1}, \ldots x_{i}^{l}]
\]</span> 其中<span class="math inline">\(t_{j}^{l}=\left[x_{j}^{r_{l}}, \ldots, x_{j}^{s_{l}}\right]\)</span>替换<span class="math inline">\(t_{i}^{k}=\)</span> <span class="math inline">\(\left[x_{i}^{r_{k}}, \ldots, x_{i}^{s_{k}}\right]\)</span>如上图1所示<em>a touching transcend love story</em> 替换<em>this poor film.</em></p>
<h4 id="treemix制作标签">TreeMix制作标签</h4>
<p>为扩充的样本<span class="math inline">\(\overline{x}\)</span>创建有效标签是一个具有挑战性的问题。类似于Mixup，我们使用原始的凸组合两个句子的标签作为扩充样本的标签。 <span class="math display">\[
\overline{\mathbf{y}}=\frac{l_{i}-\left|t_{i}^{k}\right|}{l_{i}-\left|t_{i}^{k}\right|+\left|t_{j}^{l}\right|} \mathbf{y}_{i}+\frac{\left|t_{j}^{l}\right|}{l_{i}-\left|t_{i}^{k}\right|+\left|t_{j}^{l}\right|} \mathbf{y}_{j}
\]</span> 其中<span class="math inline">\(l_i\)</span>为<span class="math inline">\(x_i\)</span>的长度，<span class="math inline">\(|t_i^k|\)</span>为子树的长度，在新的句子中,从<span class="math inline">\(x_i\)</span>中保留<span class="math inline">\(l_i-|t_i^k|\)</span>个words，从句子<span class="math inline">\(x_j\)</span>插入<span class="math inline">\(|t_j^l|\)</span>个words。</p>
<p><span class="math inline">\(\frac{l_{i}-\left|t_{i}^{k}\right|}{l_{i}-\left|t_{i}^{k}\right|+\left|t_{j}^{l}\right|}\)</span>是来自<span class="math inline">\(x_i\)</span>的words的分数，其可以决定<span class="math inline">\(y_i\)</span>的权重，然后，基于标签的变化与原始句子中的长度变化成比例的猜想来创建标签。附录提供样本。</p>
<figure>
<img src="https://cdn.jsdelivr.net/gh/zhoujiangfeng/images@main/20220614162151.png" alt="image-20220614162150953" /><figcaption aria-hidden="true">image-20220614162150953</figcaption>
</figure>
<h4 id="组合算法">组合算法</h4>
<p><img src="https://cdn.jsdelivr.net/gh/zhoujiangfeng/images@main/20220614163620.png" style="zoom:50%;" /></p>
<p>我们的主要算法如算法1所示。虽然TreeMix创建的句子<strong>并不都是流畅的甚至有效的新句子</strong>，但它们<strong>包含具有不同含义的子部分</strong>，这<strong>鼓励模型以组合的方式构建丰富的句子表示</strong>。需要注意的是，扩展后的标签是<strong>原始标签的凸组合</strong>，只有当模型学习到<strong>两个部分的表示在一起</strong>时，它们才能<strong>预测具有不同权重的两个标签</strong>。</p>
<h4 id="training-objective">Training Objective</h4>
<p>我们的模型是在原始样本和增强样本的组合上训练，以获得正则化和噪声注入之间的权衡。最终的培训目标是: <span class="math display">\[
\begin{aligned}
\mathcal{L}=&amp; \underset{(\mathbf{x}, \mathbf{y}) \sim D}{\mathbb{E}}\left[-\mathbf{y}^{\top} \log P_{\theta}(\mathbf{y} \mid \mathbf{x})\right]
+\gamma \underset{(\overline{\mathbf{x}}, \overline{\mathbf{y}}) \sim D^{\prime}}{\mathbb{E}}\left[-\overline{\mathbf{y}}^{\top} \log P_{\theta}(\overline{\mathbf{y}} \mid \overline{\mathbf{x}})\right]
\end{aligned}
\]</span> <span class="math inline">\(\gamma\)</span>i是增强样本的权重</p>
<h2 id="experiment">Experiment</h2>
<ul>
<li><p>数据集</p>
<p><img src="https://cdn.jsdelivr.net/gh/zhoujiangfeng/images@main/20220614164635.png" alt="image-20220614164635190" style="zoom:50%;" /></p></li>
<li><p><strong>Baseline</strong></p>
<table>
<thead>
<tr class="header">
<th>approaches</th>
<th>简介</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>BERT</td>
<td></td>
</tr>
<tr class="even">
<td>EDA</td>
<td>由四个简单操作组成：同义词替换、随机插入、随机交换和随机删除。</td>
</tr>
<tr class="odd">
<td>AEDA</td>
<td>在文本中随机插入标点符号的AEDA</td>
</tr>
<tr class="even">
<td>Back Translation</td>
<td>将句子翻译成临时语言(EN-DE)，然后将先前翻译的文本翻译回源语言(DE-EN)</td>
</tr>
<tr class="odd">
<td>GPT3Mix</td>
<td>设计提示并利用GPT3生成新的示例来训练模型。</td>
</tr>
<tr class="even">
<td>SSMix</td>
<td>通过在给定类的所有示例前添加类标签来为条件BART。BARTword屏蔽了单个单词，而BARTspan屏蔽了连续的区块。</td>
</tr>
<tr class="odd">
<td>EmbedMix</td>
<td></td>
</tr>
<tr class="even">
<td>Tmix</td>
<td>首先对两个输入分别编码，然后在某一编码器层a处对两个嵌入进行线性插值，最终向前传递组合嵌入到其余层中。</td>
</tr>
</tbody>
</table></li>
<li><p><strong>结果</strong></p>
<p><img src="https://cdn.jsdelivr.net/gh/zhoujiangfeng/images@main/20220614170349.png" alt="image-20220614170349622" style="zoom:50%;" /></p></li>
</ul>
<figure>
<img src="https://cdn.jsdelivr.net/gh/zhoujiangfeng/images@main/20220614170416.png" alt="image-20220614170416075" /><figcaption aria-hidden="true">image-20220614170416075</figcaption>
</figure>
<figure>
<img src="https://cdn.jsdelivr.net/gh/zhoujiangfeng/images@main/20220614170443.png" alt="image-20220614170443863" /><figcaption aria-hidden="true">image-20220614170443863</figcaption>
</figure>
<h2 id="conclusion">Conclusion</h2>
<p>没找到代码，自己摸索了。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2022/04/12/2022-06-14-%E3%80%8C%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E3%80%8D2022NAAC%EF%BC%9ATreeMix/" data-id="cl4od7abb00097fhogzs47nb4" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-2022-06-16-「pytorch」快速上手" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2022/04/12/2022-06-16-%E3%80%8Cpytorch%E3%80%8D%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B/" class="article-date">
  <time datetime="2022-04-12T08:45:27.104Z" itemprop="datePublished">2022-04-12</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="样例-快速入门pytorch">样例-快速入门Pytorch</h1>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">modellogit</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        nn.Module.__init__(self)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">2</span>,<span class="number">16</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">16</span>,<span class="number">8</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">8</span>,<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        x = torch.relu(self.fc1(x))</span><br><span class="line">        x = torch.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">hello_model</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,max_iter=<span class="number">200</span>,learning_rate=<span class="number">0.01</span>,l1=<span class="number">1e-4</span></span>):</span><br><span class="line">        self.model = modellogit()</span><br><span class="line">        self.max_iter = max_iter</span><br><span class="line">        self.learning_rate = learning_rate</span><br><span class="line">        self.l1 = l1</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">fit</span>(<span class="params">self,train_x,train_y</span>):</span><br><span class="line">        optimizer = optim.Adam(self.model.parameters(),lr = self.learning_rate)</span><br><span class="line">        criterion = nn.MSELoss()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;训练开始&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(self.max_iter):</span><br><span class="line">            input_x = torch.from_numpy(train_x).<span class="built_in">float</span>()</span><br><span class="line">            target = torch.from_numpy(train_y).<span class="built_in">float</span>()</span><br><span class="line">            <span class="comment">#梯度置零</span></span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            <span class="comment">#正向传播</span></span><br><span class="line">            output = self.model(input_x)</span><br><span class="line">            <span class="comment">#反向传播</span></span><br><span class="line">            loss = criterion(output, target)</span><br><span class="line">            regular_loss = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> param <span class="keyword">in</span> self.model.parameters():</span><br><span class="line">                regular_loss += torch.<span class="built_in">sum</span>(torch.<span class="built_in">abs</span>(param))</span><br><span class="line">            loss += self.l1*regular_loss</span><br><span class="line">            loss.backward()</span><br><span class="line">            <span class="comment">#优化</span></span><br><span class="line">            optimizer.step()</span><br><span class="line">            <span class="keyword">if</span> epoch%<span class="built_in">int</span>(self.max_iter/<span class="number">5</span>) == <span class="number">0</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&#x27;[%d, %5d] loss: %.3f&#x27;</span>%(epoch+<span class="number">1</span>, epoch+<span class="number">1</span>, loss.data))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;训练结束&#x27;</span>)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">self,test_x</span>):</span><br><span class="line">        y_hat = self.model(torch.from_numpy(test_x).<span class="built_in">float</span>())</span><br><span class="line">        y_hat = y_hat.detach()</span><br><span class="line">        y_hat = y_hat.numpy()</span><br><span class="line">        <span class="keyword">return</span>(y_hat)</span><br><span class="line">    </span><br><span class="line">data_x = np.random.normal(size = (<span class="number">30000</span>,<span class="number">2</span>))</span><br><span class="line">data_y = data_x[:,[<span class="number">0</span>]] + data_x[:,[<span class="number">0</span>]]**<span class="number">2</span> + data_x[:,[<span class="number">1</span>]]**<span class="number">2</span>+\</span><br><span class="line">    data_x[:,[<span class="number">1</span>]]*<span class="number">4</span>+np.random.normal(size = (<span class="number">30000</span>,<span class="number">1</span>))*<span class="number">0.1</span></span><br><span class="line">train_x,test_x,train_y,test_y = train_test_split(data_x,data_y,test_size=<span class="number">0.33</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line">model = hello_model(max_iter=<span class="number">5000</span>,learning_rate=<span class="number">0.001</span>)</span><br><span class="line">model.fit(train_x,train_y)</span><br><span class="line">model.predict(test_x)</span><br></pre></td></tr></table></figure>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2022/04/12/2022-06-16-%E3%80%8Cpytorch%E3%80%8D%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B/" data-id="cl4od7abc000d7fho1mh4b9fb" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-2022-04-01-simcse论文阅读笔记" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/12/26/2022-04-01-simcse%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" class="article-date">
  <time datetime="2020-12-26T08:14:36.000Z" itemprop="datePublished">2020-12-26</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA/">数据增强</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/12/26/2022-04-01-simcse%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">SimCSE：一种对比学习数据增强方法</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="title">Title</h1>
<p>SimCSE: Simple Contrastive Learning of Sentence Embeddings</p>
<h2 id="time">Time</h2>
<p>2021.9</p>
<h2 id="publish">Publish</h2>
<p>Emnlp2021</p>
<h2 id="summary">Summary</h2>
<p><img src="https://cdn.jsdelivr.net/gh/zhoujiangfeng/images@main/20220323164848.png" alt="image-20220323164848858" style="zoom:25%;" /></p>
<ol type="1">
<li>提出了目前在sentence embedding 领域表现最优的对比学习框架SimCSE</li>
<li>首先使用无监督方法，将输入sentence和其预测的sentence1作为对比目标，其只经过一个标准的dropout作为噪音，取得的很好效果，与以前的有监督效果相当。</li>
<li>其发现利用 dropout作为数据增强操作有很好的效果，且移除它会导致表示效果变差。</li>
<li>进而提出有监督方法。标注的自然语言推理数据集输入对比学习框架，使用“entailment”数据作为正样本，“contradiction”作为hard 负样本。</li>
<li>最后在STS任务评测，无监督、有监督simcse+bert_base 斯皮尔曼相关系数分别提升4.2%、2.2%</li>
<li>证明：对比学习 正则化了 预训练模型嵌入 各向异性空间变得更加统一，使用监督学习能够使得正样本更好的对齐。</li>
</ol>
<h2 id="methods">Method(s)</h2>
<h3 id="无监督simcse实现"><strong>无监督SimCSE实现</strong></h3>
<p><img src="https://cdn.jsdelivr.net/gh/zhoujiangfeng/images@main/20220323171412.png" alt="无监督SimCSE" style="zoom:25%;" /></p>
<ol type="1">
<li>将句子输入 dropout作为noise</li>
<li>将同一句输入预训练模型编码器两次：应用标准的dropout两次。获得两个不同的嵌入作为“positive pairs”</li>
<li>同一个batch其他句子，组合作为“negatives”</li>
<li>最后模型从这些“negatives”中预测“positive”</li>
</ol>
<h3 id="train-objective">train objective</h3>
<p>同样的input到编码器两次，获得两个embedding<span class="math inline">\(z\)</span>,<span class="math inline">\(z^{\prime}\)</span></p>
<p><span class="math inline">\(\ell_{i}=-\log \frac{e^{\operatorname{sim}\left(\mathbf{h}_{i}^{z_{i}}, \mathbf{h}_{i}^{z_{i}^{\prime}}\right) / \tau}}{\sum_{j=1}^{N} e^{\operatorname{sim}\left(\mathbf{h}_{i}^{z_{i}}, \mathbf{h}_{j}^{z_{j}^{\prime}}\right) / \tau}}\)</span></p>
<p><span class="math inline">\(z\)</span>仅仅使用transformer标准的dropout mask</p>
<p><strong>结论：</strong>dropout对隐藏的表示起到了最小的“数据扩充”作用，而删除它则会导致表示崩溃。</p>
<h3 id="有监督simcse实现"><strong>有监督SimCSE实现</strong></h3>
<p><img src="https://cdn.jsdelivr.net/gh/zhoujiangfeng/images@main/20220324113118.png" alt="image-20220324113118463" style="zoom:40%;" /></p>
<ul>
<li>使用自然语言推理数据集，样例如上</li>
</ul>
<p>1.将原有三分类任务（蕴含，中立，相反）降低为（蕴含，相反），其中“中立”被视为正样本。</p>
<p>2.添加对应的相反 语句对视为 hard negative 进一步提升表现。NLI数据集效果尤其好。</p>
<h3 id="train-objective-1">train objective</h3>
<p>将<span class="math inline">\(x，x_i^+\)</span>扩展为<span class="math inline">\(x，x_i^+，x_i^-\)</span>,以<span class="math inline">\(x\)</span>为基础，<span class="math inline">\(x_i^+\)</span>和<span class="math inline">\(x_i^-\)</span>为蕴含和相反 假设</p>
<p>其训练目标为</p>
<p>​ <span class="math inline">\(-\log \frac{e^{\operatorname{sim}\left(\mathbf{h}_{i}, \mathbf{h}_{i}^{+}\right) / \tau}}{\sum_{j=1}^{N}\left(e^{\operatorname{sim}\left(\mathbf{h}_{i}, \mathbf{h}_{j}^{+}\right) / \tau}+e^{\operatorname{sim}\left(\mathbf{h}_{i}, \mathbf{h}_{j}^{-}\right) / \tau}\right)}\)</span></p>
<h2 id="evaluation">Evaluation</h2>
<p>对比学习分析工具：</p>
<ol type="1">
<li><p>alignment</p>
<p>测试语义相关的正样本对齐程度</p>
<p><span class="math inline">\(\ell_{\text {align }} \triangleq \underset{\left(x, x^{+}\right) \sim p_{\text {pos }}}{\mathbb{E}}\left\|f(x)-f\left(x^{+}\right)\right\|^{2}\)</span></p>
<p>其中<span class="math inline">\(p_{pos}\)</span>表示 语句对对应的分布式的句向量，假设其中表示均已正则化</p></li>
<li><p>uniformity</p>
<p>利用整个表示空间的均匀性来衡量学习嵌入的质量</p></li>
</ol>
<p>​ <span class="math inline">\(\ell_{\text {uniform }} \triangleq \log \underset{x, y \stackrel{\mathbb{i} . i . d .}{\sim} p_{\text {data }}}{ } e^{-2\|f(x)-f(y)\|^{2}}\)</span></p>
<p>这两个指标与对比学习的目标是一致的:正面实例应该保持接近，而随机实例的嵌入应该分散在超球体上。<img src="https://cdn.jsdelivr.net/gh/zhoujiangfeng/images@main/20220324155942.png" alt="image-20220324155942504" style="zoom: 33%;" /><img src="/Users/zhoujiangfeng/Library/Application%20Support/typora-user-images/image-20220324170115756.png" alt="image-20220324170115756" style="zoom:50%;" /></p>
<p>其数字越小越好</p>
<h2 id="conclusion">Conclusion</h2>
<h6 id="section"><img src="https://cdn.jsdelivr.net/gh/zhoujiangfeng/images@main/20220324164727.png" alt="image-20220324164727102" style="zoom:50%;" /></h6>
<p><img src="https://cdn.jsdelivr.net/gh/zhoujiangfeng/images@main/20220324164933.png" alt="image-20220324164933309" style="zoom: 50%;" /></p>
<h2 id="notes">Notes</h2>
<h4 id="对比学习"><strong>对比学习</strong>：</h4>
<p><em>对比学习的目的是通过把语义上相近的邻居拉到一起，把非邻居推开来学习有效的表征</em></p>
<p>假定语句集<span class="math inline">\(\mathcal{D}=\left\{\left(x_{i}, x_{i}^{+}\right)\right\}_{i=1}^{m}\)</span>，其<span class="math inline">\(x_{i}\)</span>和<span class="math inline">\(x_{i}^{+}\)</span>语义相关</p>
<p>采用交叉熵损失</p>
<p>其<span class="math inline">\(h_{i}\)</span>和<span class="math inline">\(h_{i}^{+}\)</span>表示<span class="math inline">\(x_{i}\)</span>和<span class="math inline">\(x_{i}^{+}\)</span></p>
<p>对于（<span class="math inline">\(x_{i}\)</span>，<span class="math inline">\(x_{i}^{+}\)</span>）的Npair mini-batch 的训练目标为</p>
<p><span class="math inline">\(\ell_{i}=-\log \frac{e^{\operatorname{sim}\left(\mathbf{h}_{i}, \mathbf{h}_{i}^{+}\right) / \tau}}{\sum_{j=1}^{N} e^{\operatorname{sim}\left(\mathbf{h}_{i}, \mathbf{h}_{j}^{+}\right) / \tau}}\)</span></p>
<p>其中<span class="math inline">\(\tau\)</span>为超参数，<span class="math inline">\(\operatorname{sim}\left(\mathbf{h}_{i}, \mathbf{h}_{i}^{+}\right)\)</span>表示cos相似度<span class="math inline">\(\frac{\mathbf{h}_{1}^{\top} \mathbf{h}_{2}}{\left\|\mathbf{h}_{1}\right\| \cdot\left\|\mathbf{h}_{2}\right\|}\)</span></p>
<p><strong>使用步骤</strong></p>
<p>首先利用PLM编码输入sentence</p>
<p>然后利用上述对比学习目标fine-tune所有参数</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2020/12/26/2022-04-01-simcse%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" data-id="cl4od7aay00007fhofgm8bugx" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" rel="tag">对比学习</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA/" rel="tag">数据增强</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-2022-04-08-WhiteningBERT" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/12/26/2022-04-08-WhiteningBERT/" class="article-date">
  <time datetime="2020-12-26T08:14:36.000Z" itemprop="datePublished">2020-12-26</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="whiteningbert降低向量维度且效果提高">WhiteningBERT：降低向量维度且效果提高</h1>
<h1 id="背景">背景</h1>
<p>使用BERT等模型生成句向量，使用FAISS或ElasticSearch（ES）等引擎进行语义向量检索，是工业界常用的方法。然后在巨大的数据量时搜索时间成本巨大。</p>
<p>Whitening-BERT借鉴PCA方法对语义向量进行降维。有效提升语义相似度计算的效果。</p>
<p>首先介绍一下PCA降维降维方法。</p>
<h2 id="pca主成分分析">PCA（主成分分析）</h2>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1E5411E71z?share_source=copy_web">用最直观的方式告诉你：什么是主成分分析PCA</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2020/12/26/2022-04-08-WhiteningBERT/" data-id="cl4od7ab600017fhoclo16kh7" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  


</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">分类</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA/">数据增强</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" rel="tag">对比学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA/" rel="tag">数据增强</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签云</h3>
    <div class="widget tagcloud">
      <a href="/tags/%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/" style="font-size: 10px;">对比学习</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA/" style="font-size: 10px;">数据增强</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/06/">六月 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/04/">四月 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/12/">十二月 2020</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2022/06/21/hello-world/">Hello World</a>
          </li>
        
          <li>
            <a href="/2022/04/12/2022-04-12-SeqGAN/">(no title)</a>
          </li>
        
          <li>
            <a href="/2022/04/12/2022-05-26-%E3%80%8C%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E3%80%8D2022NAACL%EF%BC%9ATextSmoth/">(no title)</a>
          </li>
        
          <li>
            <a href="/2022/04/12/2022-05-27-%E3%80%8CGAN%E3%80%8D2021EMNLP%EF%BC%9A%E5%AF%B9LGAN%E8%BF%9B%E8%A1%8CCounter-Contrastive%E5%AD%A6%E4%B9%A0/">(no title)</a>
          </li>
        
          <li>
            <a href="/2022/04/12/2022-05-28-%E3%80%8C%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E3%80%8D2022ACL%EF%BC%9AGlitter/">(no title)</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2022 周江峰<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>




  </div>
</body>
</html>