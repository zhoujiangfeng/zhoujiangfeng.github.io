<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>都灵的夏天</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="都灵的夏天"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="都灵的夏天"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="日拱一卒，功不唐捐"><meta property="og:type" content="blog"><meta property="og:title" content="都灵的夏天"><meta property="og:url" content="http://example.com/"><meta property="og:site_name" content="都灵的夏天"><meta property="og:description" content="日拱一卒，功不唐捐"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="http://example.com/img/og_image.png"><meta property="article:author" content="周江峰"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://example.com"},"headline":"都灵的夏天","image":["http://example.com/img/og_image.png"],"author":{"@type":"Person","name":"周江峰"},"publisher":{"@type":"Organization","name":"都灵的夏天","logo":{"@type":"ImageObject","url":"http://example.com/img/logo.svg"}},"description":"日拱一卒，功不唐捐"}</script><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }
          Array
              .from(document.querySelectorAll('.tab-content'))
              .forEach($tab => {
                  $tab.classList.add('is-hidden');
              });
          Array
              .from(document.querySelectorAll('.tabs li'))
              .forEach($tab => {
                  $tab.classList.remove('is-active');
              });
          const $activeTab = document.querySelector(location.hash);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
          const $tabMenu = document.querySelector(`a[href="${location.hash}"]`);
          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.2.0"><link rel="alternate" href="/atom.xml" title="都灵的夏天" type="application/atom+xml">
</head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="都灵的夏天" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item is-active" href="/">主页</a><a class="navbar-item" href="/archives">所有文章</a><a class="navbar-item" href="/categories">目录</a><a class="navbar-item" href="/tags">标签</a><a class="navbar-item" href="/about">关于</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2022-07-02T16:00:00.000Z" title="7/3/2022, 12:00:00 AM">2022-07-03</time>发表</span><span class="level-item"><time dateTime="2022-07-02T17:00:02.592Z" title="7/3/2022, 1:00:02 AM">2022-07-03</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E5%AD%A6%E6%9C%AF%E5%9E%83%E5%9C%BE%E5%88%B6%E9%80%A0/">学术垃圾制造</a></span><span class="level-item">3 分钟读完 (大约382个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/07/03/%E7%AC%AC%E4%B8%80%E6%AC%A1%E8%AE%BA%E6%96%87%E6%8A%95%E9%80%92%E5%A4%B1%E8%B4%A5%E7%BB%8F%E5%8E%86/">第一次论文失败经历</a></h1><div class="content"><p>去年九月份，采购问答机器人项目由于缺乏数据，原始文件就几百条，导致模型准确率较低，老板催促解决这个问题，在组会上提出了数据扩充技术，下来实验，采用回译尝试了一下，遂进入数据增强方面工作，由于国庆结束，去兵科院实习了三个月，在实习之余完成了 基本实验，开年2-4月继续实验，改论文，4月初在老板的要求下投了一个CCF C类期刊，计算机应用，感觉这个名字还挺好听，当时幻想着一投就中，所以改了又改，图画了又画（全文唯一满意的就是模型图了）。然后7月第一天，下午去实验室，收到邮件，让修改后录增刊，给老板发消息，老板说不投了，再想办法。</p>
<hr>
<p>全过程</p>
<p><img src="https://cdn.jsdelivr.net/gh/zhoujiangfeng/images@main/20220703005118.png" alt="投递全过程"></p>
<p>耗费了3个月，6-7月其实光在等结果了，无心第二篇的写作。ε=(´ο｀*)))唉</p>
<p>本来6月10号三审就结束了，中间直接停滞了半个月。</p>
<p>希望下次运气好一点。</p>
<p>近期再写一篇，继续投，缓解一下焦虑。没论文在手里可太难了。都不能全身心投入工作准备中…</p>
<p>祝好运！加油</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2022-06-21T16:00:00.000Z" title="6/22/2022, 12:00:00 AM">2022-06-22</time>发表</span><span class="level-item"><time dateTime="2022-06-22T05:22:25.515Z" title="6/22/2022, 1:22:25 PM">2022-06-22</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E6%9C%AD%E8%AE%B0/">札记</a></span><span class="level-item">几秒读完 (大约73个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/06/22/%E6%8D%A2%E4%B8%BB%E9%A2%98%E5%90%8E%E7%AC%AC%E4%B8%80%E7%AF%87/">换了新主题，显示不了LaTeX公式</a></h1><div class="content"><p>昨天下午心血来潮，想换一个博客主题，然后捣腾半天，睡前弄好了。</p>
<p>文章迁移，公式又显示不了，尝试了多种方法，还是没有解决。哎</p>
<p>难道以后公式只能截图嘛😭</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2022-06-15T16:00:00.000Z" title="6/16/2022, 12:00:00 AM">2022-06-16</time>发表</span><span class="level-item"><time dateTime="2022-06-22T03:47:57.737Z" title="6/22/2022, 11:47:57 AM">2022-06-22</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/code/">code</a></span><span class="level-item">2 分钟读完 (大约311个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/06/16/%E3%80%8Cpytorch%E3%80%8D%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B/">样例-快速入门Pytorch</a></h1><div class="content"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">modellogit</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        nn.Module.__init__(self)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">2</span>,<span class="number">16</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">16</span>,<span class="number">8</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">8</span>,<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        x = torch.relu(self.fc1(x))</span><br><span class="line">        x = torch.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">hello_model</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,max_iter=<span class="number">200</span>,learning_rate=<span class="number">0.01</span>,l1=<span class="number">1e-4</span></span>):</span><br><span class="line">        self.model = modellogit()</span><br><span class="line">        self.max_iter = max_iter</span><br><span class="line">        self.learning_rate = learning_rate</span><br><span class="line">        self.l1 = l1</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">fit</span>(<span class="params">self,train_x,train_y</span>):</span><br><span class="line">        optimizer = optim.Adam(self.model.parameters(),lr = self.learning_rate)</span><br><span class="line">        criterion = nn.MSELoss()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;训练开始&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(self.max_iter):</span><br><span class="line">            input_x = torch.from_numpy(train_x).<span class="built_in">float</span>()</span><br><span class="line">            target = torch.from_numpy(train_y).<span class="built_in">float</span>()</span><br><span class="line">            <span class="comment">#梯度置零</span></span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            <span class="comment">#正向传播</span></span><br><span class="line">            output = self.model(input_x)</span><br><span class="line">            <span class="comment">#反向传播</span></span><br><span class="line">            loss = criterion(output, target)</span><br><span class="line">            regular_loss = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> param <span class="keyword">in</span> self.model.parameters():</span><br><span class="line">                regular_loss += torch.<span class="built_in">sum</span>(torch.<span class="built_in">abs</span>(param))</span><br><span class="line">            loss += self.l1*regular_loss</span><br><span class="line">            loss.backward()</span><br><span class="line">            <span class="comment">#优化</span></span><br><span class="line">            optimizer.step()</span><br><span class="line">            <span class="keyword">if</span> epoch%<span class="built_in">int</span>(self.max_iter/<span class="number">5</span>) == <span class="number">0</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&#x27;[%d, %5d] loss: %.3f&#x27;</span>%(epoch+<span class="number">1</span>, epoch+<span class="number">1</span>, loss.data))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;训练结束&#x27;</span>)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">self,test_x</span>):</span><br><span class="line">        y_hat = self.model(torch.from_numpy(test_x).<span class="built_in">float</span>())</span><br><span class="line">        y_hat = y_hat.detach()</span><br><span class="line">        y_hat = y_hat.numpy()</span><br><span class="line">        <span class="keyword">return</span>(y_hat)</span><br><span class="line">    </span><br><span class="line">data_x = np.random.normal(size = (<span class="number">30000</span>,<span class="number">2</span>))</span><br><span class="line">data_y = data_x[:,[<span class="number">0</span>]] + data_x[:,[<span class="number">0</span>]]**<span class="number">2</span> + data_x[:,[<span class="number">1</span>]]**<span class="number">2</span>+\</span><br><span class="line">    data_x[:,[<span class="number">1</span>]]*<span class="number">4</span>+np.random.normal(size = (<span class="number">30000</span>,<span class="number">1</span>))*<span class="number">0.1</span></span><br><span class="line">train_x,test_x,train_y,test_y = train_test_split(data_x,data_y,test_size=<span class="number">0.33</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line">model = hello_model(max_iter=<span class="number">5000</span>,learning_rate=<span class="number">0.001</span>)</span><br><span class="line">model.fit(train_x,train_y)</span><br><span class="line">model.predict(test_x)</span><br></pre></td></tr></table></figure>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2022-06-13T16:00:00.000Z" title="6/14/2022, 12:00:00 AM">2022-06-14</time>发表</span><span class="level-item"><time dateTime="2022-06-22T05:11:00.007Z" title="6/22/2022, 1:11:00 PM">2022-06-22</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/DA/">DA</a></span><span class="level-item">15 分钟读完 (大约2258个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/06/14/%E3%80%8C%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E3%80%8D2022NAAC%EF%BC%9ATreeMix/">TreeMix：基于组合的数据增强方法</a></h1><div class="content"><h2 id="Publish"><a href="#Publish" class="headerlink" title="Publish"></a>Publish</h2><p>NAACL2022</p>
<h2 id="Title"><a href="#Title" class="headerlink" title="Title"></a>Title</h2><p>TreeMix: Compositional Constituency-based Data Augmentation for Natural Language Understanding</p>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>数据增强是解决过度拟合问题的一种有效方法。前人针对自然语言处理提出了不同的数据扩充策略，如噪声注入、单词替换、回译等。虽然有效，但它们<strong>忽略了语言的一个重要特征-组合性</strong>，<strong>复杂表达的意义是从其子部分建立起来的</strong>。受此启发，我们提出了一种用于自然语言理解的成分数据扩充方法TreeMix。具体地说，<strong>TreeMix利用选区分析树将句子分解成构成子结构，并利用Mixup数据增强技术对它们进行重组以生成新的句子</strong>。与以前的方法相比，TreeMix在生成的样本中引入了更大的<strong>多样性</strong>，并鼓励模型学习NLP数据的<strong>组合性</strong>。在文本分类和SCAN的大量实验表明，TreeMix的性能优于目前最先进的数据增强方法。</p>
<h2 id="Solution-problem"><a href="#Solution-problem" class="headerlink" title="Solution problem"></a>Solution problem</h2><p>合成性是语言的一个关键方面，因为复句的意义是从它的子部分建立起来的。先前的工作还表明，语法树(例如，基于树的LSTM)有助于对句子结构进行建模，以便更好地进行文本分类。然而，在语言技术社区中，除了在语义分析方面的一些例外情况外，利用组合结构进行数据扩充并没有受到太多关注</p>
<p><img src="https://cdn.jsdelivr.net/gh/zhoujiangfeng/images@main/20220613135323.png" alt="image-20220613135315056"></p>
<p>我们提出了一种用于自然语言理解的成分数据增强方法，即TreeMix(图1)。TreeMix是一种输入级混合方法，它利用成分分析信息，将来自不同句子的不同片段(子树的短语)进行重组，以创建训练集中从未见过的新示例；同时还将基于这些片段策略性地创建新的软标签。这样，TreeMix不仅利用了构成语言的特征来增加扩充的多样性，而且为这些混合的例子提供了合理的软标签。</p>
<p>mixup定义为</p>
<script type="math/tex; mode=display">
\tilde{x}=\lambda x_{i}+(1-\lambda) x_{j}</script><script type="math/tex; mode=display">
\tilde{y}=\lambda y_{i}+(1-\lambda) y_{j}</script><p>其中$(x_i,x_j),(y_i,y_j)$为从训练数据中随机抽出两个目标特征向量,$\lambda\in[0,1]$</p>
<p>我们通过融入语言的组合性来改进Mixup，这是泛化所必需的一个关键特征，但神经模型往往无法捕捉到这一点。我们新提出的方法TreeMix不是用整个样本进行内插，而是<strong>通过删除句子的短语并重新插入其他句子的子部分来创建新句子</strong>。TreeMix利用选民树将句子分解成有意义的组成部分，然后将这些组成部分移除并重新组合，以生成新的扩充样本。我们的目标是通过对TreeMix生成的大量样本进行训练来提高模型的组合性泛化能力。一个使用TreeMix进行单句分类的例子如上图所示。</p>
<h3 id="TreeMix-详细过程"><a href="#TreeMix-详细过程" class="headerlink" title="TreeMix 详细过程"></a>TreeMix 详细过程</h3><p>${x}<em>{i}=\left{x</em>{i}^{1}, x<em>{i}^{2}, \ldots, x</em>{i}^{l}\right}$表示长度为$l$的序列，其对应的one-hot编码label为$y<em>i$,我们在$x_i$上运行一个解析器得到它的解析树$T(x_i)$,为了获取序列中有意义的子部分，采用递归遍历解析树，获得所有具有一个以上child的子树。表示子树的集合为$S(x_i)= {t_i^k}$.其中$t_i^k$表示样本$x_i$的第k个子树，对于子树$t_i^k$连续覆盖了$x_i$的$t</em>{i}^{k} \triangleq\left[x<em>{i}^{r</em>{k} }, \ldots, x<em>{i}^{s</em>{k} }\right]$,索引$r_k$为开始，$s_k$为结束。例如图一左侧所示，例句子树可以cover span 的有1.<em>this poor film</em>,2. <em>in this poor film</em>, 3.<em>no interest …etc</em></p>
<p><img src="https://cdn.jsdelivr.net/gh/zhoujiangfeng/images@main/20220614152117.png" style="zoom:50%;" /></p>
<p>对于给定的样本$(x_i,y_i)$，我们从训练集中随机抽取另一个数据点$(x_j, y_j)$。我们对这两个句子运行选区解析器，得到它们的子树集$S(x_i)$和$S(x_j)$，我们可以对要交换的子树进行采样。我们引入两个额外的超参数$λ_L$和$λ_U$来约束待采样子树的长度。$λ_L$和$λ_U$，用子树与原始句子的长度之比来衡量要采样子树的上下限。直观地说，$λ$控制着我们想要交换的短语的粒度。我们希望交换的长度是合理的。如果它太短，那么交换不能给增强样本引入足够的多样性;否则，如果太长，这个过程可能会给原句注入太多噪音。我们设置λ为比率，以便与原句子的长度不变。表2显示了一些具有不同长度约束的子树示例。我们将长度受限子树集合定义为:</p>
<script type="math/tex; mode=display">
S_{\lambda}(\mathbf{x}) \triangleq\left\{t \mid t \in S(\mathbf{x})\right., s.t. \left.\frac{|t|}{|\mathbf{x}|} \in\left[\lambda_{L}, \lambda_{U}\right]\right\}</script><p>其中$|·|$表示序列或子树的长度，对于两个句子$x<em>i$和$x_j$,我们随机采样两个子树$t</em>{i}^{k} \in S<em>{\lambda}\left(\mathbf{x}</em>{i}\right)$和$t<em>{j}^{l} \in S</em>{\lambda}\left(\mathbf{x}_{j}\right)$并且通过$t_j^l$替换$t_i^k$构建新的样本。例如</p>
<script type="math/tex; mode=display">
\overline{\mathbf{x}} \triangleq[x_{i}^{1}, \ldots, x_{i}^{r_{k}-1}, \underbrace{x_{j}^{r_{l}}, \ldots, x_{j}^{s_{l}}}_{t_{j}^{l}}, x_{i}^{s_{k}+1}, \ldots x_{i}^{l}]</script><p>其中$t<em>{j}^{l}=\left[x</em>{j}^{r<em>{l} }, \ldots, x</em>{j}^{s<em>{l} }\right]$替换$t</em>{i}^{k}=$ $\left[x<em>{i}^{r</em>{k}}, \ldots, x<em>{i}^{s</em>{k} }\right]$如上图1所示<em>a touching transcend love story</em> 替换<em>this poor film.</em></p>
<h4 id="TreeMix制作标签"><a href="#TreeMix制作标签" class="headerlink" title="TreeMix制作标签"></a>TreeMix制作标签</h4><p>为扩充的样本$\overline{x}$创建有效标签是一个具有挑战性的问题。类似于Mixup，我们使用原始的凸组合两个句子的标签作为扩充样本的标签。</p>
<script type="math/tex; mode=display">
\overline{\mathbf{y}}=\frac{l_{i}-\left|t_{i}^{k}\right|}{l_{i}-\left|t_{i}^{k}\right|+\left|t_{j}^{l}\right|} \mathbf{y}_{i}+\frac{\left|t_{j}^{l}\right|}{l_{i}-\left|t_{i}^{k}\right|+\left|t_{j}^{l}\right|} \mathbf{y}_{j}</script><p>其中$l_i$为$x_i$的长度，$|t_i^k|$为子树的长度，在新的句子中,从$x_i$中保留$l_i-|t_i^k|$个words，从句子$x_j$插入$|t_j^l|$个words。</p>
<p>$\frac{l<em>{i}-\left|t</em>{i}^{k}\right|}{l<em>{i}-\left|t</em>{i}^{k}\right|+\left|t_{j}^{l}\right|}$是来自$x_i$的words的分数，其可以决定$y_i$的权重，然后，基于标签的变化与原始句子中的长度变化成比例的猜想来创建标签。附录提供样本。</p>
<p><img src="https://cdn.jsdelivr.net/gh/zhoujiangfeng/images@main/20220614162151.png" alt="image-20220614162150953"></p>
<h4 id="组合算法"><a href="#组合算法" class="headerlink" title="组合算法"></a>组合算法</h4><p><img src="https://cdn.jsdelivr.net/gh/zhoujiangfeng/images@main/20220614163620.png" style="zoom:50%;" /></p>
<p>我们的主要算法如算法1所示。虽然TreeMix创建的句子<strong>并不都是流畅的甚至有效的新句子</strong>，但它们<strong>包含具有不同含义的子部分</strong>，这<strong>鼓励模型以组合的方式构建丰富的句子表示</strong>。需要注意的是，扩展后的标签是<strong>原始标签的凸组合</strong>，只有当模型学习到<strong>两个部分的表示在一起</strong>时，它们才能<strong>预测具有不同权重的两个标签</strong>。</p>
<h4 id="Training-Objective"><a href="#Training-Objective" class="headerlink" title="Training Objective"></a>Training Objective</h4><p>我们的模型是在原始样本和增强样本的组合上训练，以获得正则化和噪声注入之间的权衡。最终的培训目标是:</p>
<script type="math/tex; mode=display">
\begin{aligned}
\mathcal{L}=& \underset{(\mathbf{x}, \mathbf{y}) \sim D}{\mathbb{E}}\left[-\mathbf{y}^{\top} \log P_{\theta}(\mathbf{y} \mid \mathbf{x})\right]
+\gamma \underset{(\overline{\mathbf{x} }, \overline{\mathbf{y} }) \sim D^{\prime} }{\mathbb{E}}\left[-\overline{\mathbf{y} }^{\top} \log P_{\theta}(\overline{\mathbf{y} } \mid \overline{\mathbf{x} })\right]
\end{aligned}</script><p>$\gamma$i是增强样本的权重</p>
<h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><ul>
<li><p>数据集</p>
<p><img src="https://cdn.jsdelivr.net/gh/zhoujiangfeng/images@main/20220614164635.png" alt="image-20220614164635190" style="zoom:50%;" /></p>
</li>
</ul>
<ul>
<li><p><strong>Baseline</strong></p>
<p>| approaches       | 简介                                                         |<br>| ———————— | —————————————————————————————— |<br>| BERT             |                                                              |<br>| EDA              | 由四个简单操作组成：同义词替换、随机插入、随机交换和随机删除。 |<br>| AEDA             | 在文本中随机插入标点符号的AEDA                               |<br>| Back Translation | 将句子翻译成临时语言(EN-DE)，然后将先前翻译的文本翻译回源语言(DE-EN) |<br>| GPT3Mix          | 设计提示并利用GPT3生成新的示例来训练模型。                   |<br>| SSMix            | 通过在给定类的所有示例前添加类标签来为条件BART。BARTword屏蔽了单个单词，而BARTspan屏蔽了连续的区块。 |<br>| EmbedMix         |                                                              |<br>| Tmix             | 首先对两个输入分别编码，然后在某一编码器层a处对两个嵌入进行线性插值，最终向前传递组合嵌入到其余层中。 |</p>
</li>
<li><p><strong>结果</strong></p>
<p><img src="https://cdn.jsdelivr.net/gh/zhoujiangfeng/images@main/20220614170349.png" alt="image-20220614170349622" style="zoom:50%;" /></p>
</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/zhoujiangfeng/images@main/20220614170416.png" alt="image-20220614170416075"></p>
<p><img src="https://cdn.jsdelivr.net/gh/zhoujiangfeng/images@main/20220614170443.png" alt="image-20220614170443863"></p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>没找到代码，自己摸索了。</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2022-05-26T16:00:00.000Z" title="5/27/2022, 12:00:00 AM">2022-05-27</time>发表</span><span class="level-item"><time dateTime="2022-06-22T03:45:48.471Z" title="6/22/2022, 11:45:48 AM">2022-06-22</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/GAN/">GAN</a></span><span class="level-item">2 分钟读完 (大约358个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/05/27/%E3%80%8CGAN%E3%80%8D2021EMNLP%EF%BC%9A%E5%AF%B9LGAN%E8%BF%9B%E8%A1%8CCounter-Contrastive%E5%AD%A6%E4%B9%A0/">Counter-Contrastive 学习：训练Language GAN</a></h1><div class="content"><h2 id="Publish"><a href="#Publish" class="headerlink" title="Publish"></a>Publish</h2><p>2021EMNLP</p>
<h2 id="title"><a href="#title" class="headerlink" title="title"></a>title</h2><p><a target="_blank" rel="noopener" href="https://aclanthology.org/2021.findings-emnlp.415.pdf">Counter-Contrastive Learning for Language GANs</a></p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>生成对抗网络(GANs)在图像合成方面取得了巨大的成功，但在生成自然语言方面存在一定的困难。挑战来自于鉴别器传递的uninformative learning signal。换句话说，<strong>糟糕的learning singnal限制了生成结构丰富、语义丰富的语言的学习能力</strong>。在本文中，我们提出在语言gan中采用反对比学习(CCL)方法来支持生成器的训练。与标准的GANs采用简单的二元分类器来区分样本的真假相比，我们采用了一种反对比学习信号，通过提高语言合成器的训练</p>
<ul>
<li>(1)把生成样本与真实样本 放在一起（以生成真实的数据）</li>
<li>(2)推开真实的样本（以阻碍鉴别的训练）从而防止鉴别器被过度训练。</li>
</ul>
<p>我们在合成基准和实际基准上评估了我们的方法，并与以前的GAN相比，在对抗序列生成方面产生了具有竞争力的性能。</p>
<h2 id="Solution-problem"><a href="#Solution-problem" class="headerlink" title="Solution problem"></a>Solution problem</h2><p>CCL</p>
<script type="math/tex; mode=display">
\mathcal{L}_{i}=-\log \frac{e^{\operatorname{sim}\left(\mathbf{h}_{i}, \mathbf{h}_{i}^{-}\right) / \tau}}{\sum_{j=1}^{N}\left(e^{\operatorname{sim}\left(\mathbf{h}_{j}, \mathbf{h}_{j}^{-}\right) / \tau}+e^{\operatorname{sim}\left(\mathbf{h}_{j}, \mathbf{h}_{j}^{+}\right) / \tau}\right)}</script><h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>没代码，不看了。</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2022-05-26T16:00:00.000Z" title="5/27/2022, 12:00:00 AM">2022-05-27</time>发表</span><span class="level-item"><time dateTime="2022-06-22T03:46:12.369Z" title="6/22/2022, 11:46:12 AM">2022-06-22</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/GAN/">GAN</a></span><span class="level-item">13 分钟读完 (大约1997个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/05/27/%E3%80%8CGAN%E3%80%8D%E5%90%84%E7%A7%8DGAN%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93%E5%AF%B9%E6%AF%94/">GAN原理总结及对比</a></h1><div class="content"><h2 id="原始GAN"><a href="#原始GAN" class="headerlink" title="原始GAN"></a>原始GAN</h2><p>GAN的主要灵感来源于博弈论中零和博弈的思想，应用到深度学习神经网络上来说，就是通过生成网络G（Generator）和判别网络D（Discriminator）不断博弈，进而使G学习到数据的分布，如果用到图片生成上，则训练完成后，G可以从一段随机数中生成逼真的图像。G， D的主要功能是：</p>
<p>●  G是一个生成式的网络，它接收一个随机的噪声z（随机数），通过这个噪声生成图像 </p>
<p>●  D是一个判别网络，判别一张图片是不是“真实的”。它的输入参数是x，x代表一张图片，输出D（x）代表x为真实图片的概率，如果为1，就代表100%是真实的图片，而输出为0，就代表不可能是真实的图片</p>
<p>训练过程中，生成网络G的目标就是尽量生成真实的图片去欺骗判别网络D。而D的目标就是尽量辨别出G生成的假图像和真实的图像。这样，G和D构成了一个动态的“博弈过程”，最终的平衡点即纳什均衡点。</p>
<h3 id="GAN的特点："><a href="#GAN的特点：" class="headerlink" title="GAN的特点："></a>GAN的特点：</h3><p>●  相比较传统的模型，他存在两个不同的网络，而不是单一的网络，并且训练方式采用的是对抗训练方式</p>
<p>●  GAN中G的梯度更新信息来自判别器D，而不是来自数据样本</p>
<h3 id="GAN-的优点："><a href="#GAN-的优点：" class="headerlink" title="GAN 的优点："></a>GAN 的优点：</h3><p>（以下部分摘自ian goodfellow 在Quora的问答）</p>
<p>●  GAN是一种生成式模型，相比较其他生成模型（玻尔兹曼机和GSNs）只用到了反向传播,而不需要复杂的马尔科夫链</p>
<p>●  相比其他所有模型, GAN可以产生更加清晰，真实的样本</p>
<p>●  GAN采用的是一种无监督的学习方式训练，可以被广泛用在无监督学习和半监督学习领域</p>
<p>●  相比于变分自编码器, GANs没有引入任何决定性偏置( deterministic bias),变分方法引入决定性偏置,因为他们优化对数似然的下界,而不是似然度本身,这看起来导致了VAEs生成的实例比GANs更模糊</p>
<p>●  相比VAE, GANs没有变分下界,如果鉴别器训练良好,那么生成器可以完美的学习到训练样本的分布.换句话说,GANs是渐进一致的,但是VAE是有偏差的</p>
<p>●  GAN应用到一些场景上，比如图片风格迁移，超分辨率，图像补全，去噪，避免了损失函数设计的困难，不管三七二十一，只要有一个的基准，直接上判别器，剩下的就交给对抗训练了。</p>
<h3 id="GAN的缺点："><a href="#GAN的缺点：" class="headerlink" title="GAN的缺点："></a>GAN的缺点：</h3><p>●  训练GAN需要达到纳什均衡,有时候可以用梯度下降法做到,有时候做不到.我们还没有找到很好的达到纳什均衡的方法,所以训练GAN相比VAE或者PixelRNN是不稳定的,但我认为在实践中它还是比训练玻尔兹曼机稳定的多</p>
<p>●  GAN不适合处理离散形式的数据，比如文本</p>
<p>●  GAN存在训练不稳定、梯度消失、模式崩溃的问题（目前已解决）</p>
<p>模式崩溃(model collapse)原因</p>
<p>一般出现在GAN训练不稳定的时候，具体表现为生成出来的结果非常差，但是即使加长训练时间后也无法得到很好的改善。</p>
<p>具体原因可以解释如下：GAN采用的是对抗训练的方式，G的梯度更新来自D，所以G生成的好不好，得看D怎么说。具体就是G生成一个样本，交给D去评判，D会输出生成的假样本是真样本的概率（0-1），相当于告诉G生成的样本有多大的真实性，G就会根据这个反馈不断改善自己，提高D输出的概率值。但是如果某一次G生成的样本可能并不是很真实，但是D给出了正确的评价，或者是G生成的结果中一些特征得到了D的认可，这时候G就会认为我输出的正确的，那么接下来我就这样输出肯定D还会给出比较高的评价，实际上G生成的并不怎么样，但是他们两个就这样自我欺骗下去了，导致最终生成结果缺失一些信息，特征不全。</p>
<p>关于梯度消失的问题可以参考郑华滨的令人拍案叫绝的wassertein GAN，里面给出了详细的解释，不过多重复<br><img src="https://cdn.jsdelivr.net/gh/zhoujiangfeng/images@main/20220527212456.png" alt="img"></p>
<p>局部极小值点</p>
<p>原始GAN中判别器要最小化如下损失函数，尽可能把真实样本分为正例，生成样本分为负例：</p>
<p>$-\mathbb{E}<em>{x \sim P</em>{r}}[\log D(x)]-\mathbb{E}<em>{x \sim P</em>{g}}[\log (1-D(x))]$（公式1 )</p>
<p>其中<img src="https://zhihu.com/equation?tex=P_r" alt="P_r">是真实样本分布，<img src="https://zhihu.com/equation?tex=P_g" alt="P_g">是由生成器产生的样本分布。对于生成器，Goodfellow一开始提出来一个损失函数，后来又提出了一个改进的损失函数，分别是</p>
<p>$\mathbb{E}<em>{x \sim P</em>{g}}[\log (1-D(x))]$（公式2)</p>
<p>$\mathbb{E}<em>{x \sim P</em>{g}}[-\log D(x)]$（公式3）</p>
<h3 id="为什么GAN不适合处理文本数据"><a href="#为什么GAN不适合处理文本数据" class="headerlink" title="为什么GAN不适合处理文本数据"></a>为什么GAN不适合处理文本数据</h3><ol>
<li><p>文本数据相比较图片数据来说是离散的，因为对于文本来说，通常需要将一个词映射为一个高维的向量，最终预测的输出是一个one-hot向量，假设softmax的输出是（0.2， 0.3， 0.1，0.2，0.15，0.05）那么变为onehot是（0，1，0，0，0，0），如果softmax输出是（0.2， 0.25， 0.2， 0.1，0.15，0.1 ），one-hot仍然是（0， 1， 0， 0， 0， 0），所以对于生成器来说，G输出了不同的结果但是D给出了同样的判别结果，并不能将梯度更新信息很好的传递到G中去，所以D最终输出的判别没有意义。</p>
</li>
<li><p>另外就是GAN的损失函数是JS散度，JS散度不适合衡量不想交分布之间的距离。</p>
</li>
</ol>
<p>（WGAN虽然使用wassertein距离代替了JS散度，但是在生成文本上能力还是有限，GAN在生成文本上的应用有seq-GAN,和强化学习结合的产物）</p>
<h3 id="训练GAN的一些技巧"><a href="#训练GAN的一些技巧" class="headerlink" title="训练GAN的一些技巧"></a>训练GAN的一些技巧</h3><ol>
<li><p>输入规范化到（-1，1）之间，最后一层的激活函数使用tanh（BEGAN除外）</p>
</li>
<li><p>使用wassertein GAN的损失函数，</p>
</li>
<li><p>如果有标签数据的话，尽量使用标签，也有人提出使用反转标签效果很好，另外使用标签平滑，单边标签平滑或者双边标签平滑</p>
</li>
<li><p>使用mini-batch norm， 如果不用batch norm 可以使用instance norm 或者weight norm</p>
</li>
<li><p>避免使用RELU和pooling层，减少稀疏梯度的可能性，可以使用leakrelu激活函数</p>
</li>
<li><p>优化器尽量选择ADAM，学习率不要设置太大，初始1e-4可以参考，另外可以随着训练进行不断缩小学习率，</p>
</li>
<li><p>给D的网络层增加高斯噪声，相当于是一种正则</p>
</li>
</ol>
<hr>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/Gavinmiaoc/article/details/79947877">来源</a></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2022-05-25T16:00:00.000Z" title="5/26/2022, 12:00:00 AM">2022-05-26</time>发表</span><span class="level-item"><time dateTime="2022-06-22T03:44:58.695Z" title="6/22/2022, 11:44:58 AM">2022-06-22</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/DA/">DA</a></span><span class="level-item">6 分钟读完 (大约942个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/05/26/%E3%80%8C%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E3%80%8D2022NAACL%EF%BC%9ATextSmoth/">Text Smoothing：一种数据结合mix-up的数据增强方法</a></h1><div class="content"><h2 id="Publish"><a href="#Publish" class="headerlink" title="Publish"></a>Publish</h2><p>2022ACL</p>
<h2 id="title"><a href="#title" class="headerlink" title="title"></a>title</h2><p>Text Smoothing: Enhance Various Data Augmentation Methods on Text Classification Tasks</p>
<h2 id="solution-problem"><a href="#solution-problem" class="headerlink" title="solution problem"></a>solution problem</h2><p>进入神经网络之前，标记通常被转换为对应的One-hot表示，这是词汇表的离散分布。平滑表示是从预先训练的MLM中获得候选token的概率，它可以被视为对onr-hot表示的更多信息的替代。我们提出了一种有效的数据增强方法，称为文本平滑，通过将句子从其one-hot表示转换为可控的平滑表示。我们在低资源条件下对不同基准的文本平滑进行了评估。实验结果表明，文本平滑方法的性能明显优于各种主流数据增强方法。此外，文本平滑可以与这些数据增强方法相结合，以获得更好的性能。</p>
<p><img src="https://cdn.jsdelivr.net/gh/zhoujiangfeng/images@main/20220526171229.png" alt="image-20220526171221247" style="zoom:50%;" /></p>
<p><img src="https://cdn.jsdelivr.net/gh/zhoujiangfeng/images@main/20220526175945.png" alt="image-20220526175945737" style="zoom:50%;" /></p>
<p>文本平滑代码：Pytorch</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sentence = <span class="string">&quot;My favorite fruit is pear .&quot;</span></span><br><span class="line">lambd = <span class="number">0.1</span> <span class="comment"># interpolation hyperparameter</span></span><br><span class="line">mlm.train() <span class="comment"># enable dropout, dynamically mask</span></span><br><span class="line">tensor_input = tokenizer(sentence, return_tensors=<span class="string">&quot;pt&quot;</span>)</span><br><span class="line">onehot_repr = convert_to_onehot(**tensor_input)</span><br><span class="line">smoothed_repr = softmax(mlm(**tensor_input).logits[<span class="number">0</span>])</span><br><span class="line">interpolated_repr = lambd * onehot_repr + (<span class="number">1</span> - lambd) * smoothed_repr</span><br></pre></td></tr></table></figure>
<ol>
<li><p>使用BERT作为MLM，给定下游数据集命名为：$D={t<em>i,p_i,s_i,l_i}</em>{i=1}^{N}$ ,N表示样本数量，$t_i$表示文本one-hot 编码，$p_i$表示$t_i$位置编码，$s_i$表示$t_i$的段编码，$l_i$表示实例标签。</p>
</li>
<li><p>将$t_i,p_i,s_i$送入BERT</p>
</li>
<li><p>取回BERT中Transformer-encoder最后一层的输出表示为</p>
<script type="math/tex; mode=display">
\overrightarrow{t_i}=BERT(t_i)</script><p>其中$\overrightarrow{t_i}$形状为[seq_len,emb_size]</p>
</li>
<li><p>然后乘以$\overrightarrow{t_i}$乘以BERT中词嵌入矩阵$W$,其形状为[vocab_size,embed_size]</p>
<script type="math/tex; mode=display">
MLM(t_i)=softmax(\overrightarrow(t_i)W^T)</script><p>其中$MLM(t_i)$中每一行是token词汇表中的概率分布，表示了预训练BERT学习到输入文本所在位置的包含上下文 的标记选项（信息）。</p>
</li>
<li></li>
</ol>
<ol>
<li><p>mixup定义为</p>
<script type="math/tex; mode=display">
\tilde{x}=\lambda x_{i}+(1-\lambda) x_{j}</script><script type="math/tex; mode=display">
\tilde{y}=\lambda y_{i}+(1-\lambda) y_{j}</script><p>其中$(x_i,x_j),(y_i,y_j)$为从训练数据中随机抽出两个目标特征向量,$\lambda\in[0,1]$在文本平滑中，One-hot表示和平滑表示来自相同的原始输入，标签相同，其内部插入操作不会改变标签，因此mixup操作可以简化为</p>
<script type="math/tex; mode=display">
\widetilde{t_{i}}=\lambda \cdot t_{i}+(1-\lambda) \cdot \operatorname{MLM}\left(t_{i}\right)</script><p>其中$t_i$为one-hot表示，$MLM(t_i)$为平滑表示，$\widetilde{t_i}$为联合插入表示，$\lambda$为用于控制插入的超参数。下游任务中我们使用联合表示代替one-hot变化表示作为输入。</p>
</li>
</ol>
<h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><ul>
<li><p>数据集</p>
<p><img src="https://cdn.jsdelivr.net/gh/zhoujiangfeng/images@main/20220526201719.png" alt="image-20220526201719743" style="zoom:50%;" /></p>
</li>
</ul>
<ul>
<li><p>Baseline</p>
<p>| approaches              | 简介                                                         |<br>| ———————————- | —————————————————————————————— |<br>| EDA                     | 由四个简单操作组成：同义词替换、随机插入、随机交换和随机删除。 |<br>| Back Translation        | 将句子翻译成临时语言(EN-DE)，然后将先前翻译的文本翻译回源语言(DE-EN) |<br>| CBERT                   | 用预先训练的BERT mask一些标记并预测它们的上下文替换。        |<br>| BERTexpand, BERTprepend | 通过在给定类的所有示例中添加类标签来满足BERT条件。“expand”标签以 模拟 词汇表，而“prepend”则没有 |<br>| GPT2context             | 为预先训练的GPT模型提供提示，并持续生成，直到[EOS]token      |<br>| BARTword, BARTspan      | 通过在给定类的所有示例前添加类标签来为条件BART。BARTword屏蔽了单个单词，而BARTspan屏蔽了连续的区块。 |</p>
</li>
<li><p>结果</p>
<p><img src="https://cdn.jsdelivr.net/gh/zhoujiangfeng/images@main/20220526202740.png" alt="image-20220526202740487"></p>
</li>
</ul>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>小数据，可控，目前优于其他，未来，结合其他DA=顶会。我也想发顶会啊。</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2022-04-30T16:00:00.000Z" title="5/1/2022, 12:00:00 AM">2022-05-01</time>发表</span><span class="level-item"><time dateTime="2022-06-22T03:43:20.801Z" title="6/22/2022, 11:43:20 AM">2022-06-22</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E7%A0%94%E7%A9%B6%E7%82%B9/">研究点</a></span><span class="level-item">5 分钟读完 (大约692个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/05/01/%E7%AC%AC%E4%BA%8C%E7%AF%87%E5%B0%8F%E8%AE%BA%E6%96%87%E7%9A%84%E6%80%9D%E8%80%83/">第二篇小论文的思考记录</a></h1><div class="content"><p>大论文的开题核心为”问句语义识别应用研究“。</p>
<p>第一篇小论文采用预训练模型进行数据增强以提升文本表征能力，主要研究集中在问句识别前期工作，并没有聚焦到语义识别。</p>
<p>对中文论文进行调研，大部分paper都将问句语义识别定义为一个文本分类问题，即问句分类。</p>
<p>所以第二篇小论文优先考虑<strong>文本分类问题</strong></p>
<p>近期又不可避免的 看了写GAN的文章，生成对抗网络，用于图像生成的确很不错。但是在nlp效果很一般，唯一给我的感觉就是这是一个很有学术价值的东西，适合写论文。</p>
<p><strong>seqGAN: sequence generative adversarial nets with policy gradient</strong></p>
<p>2017年发表 代码比较老  判别器用cnn，生成器rnn，改进点没想法，换成transformer？</p>
<p><strong>TT-gan:Text-to-Text Generative Adversarial Networks</strong>  2018年 模型不但可以生成真实文本，还能生成源文本释义或语义摘要。作者说是第一个 语义层面上生成自然语言的框框架。 <em>无代码</em>    没想法</p>
<p>重点来了“GAN-BERT”</p>
<p><strong>GAN-BERT: Generative Adversarial Learning for Robust Text Classification with a Bunch of Labeled Examples</strong> 2020acl。这篇文献我详细阅读了。 其判别器和生成器都是多层感知机，输入 noise（向量h） 进入生成器，输出$h_{fake}$    然后在将真实数据 输入bert 获得向量 进行判别器。设置一个多分类任务进行训练，设置k+1个类，其中k为真实的类，k+1为生成器产生的类。过程如下：</p>
<ol>
<li>经过bert的向量设为$h<em>{cls}$ ，训练目标： 判别器真实样本将其归为k类，$h</em>{fake}$归为k类</li>
<li>反向传播阶段， 无监督学习：无标签数据被错误归入k+1类时 优化 判别器的损失。有监督学习， 通同1要求 优化判别器损失。</li>
<li>训练结束，丢弃生成器，利用原始bert进行推理。（应该是bert+判别器）</li>
</ol>
<p>我的想法 能不能 用其他model替换mlp。（但是原文的一个创新就是 没有使用cnn），然后融入一些其他trick，提升分类效果。</p>
<p>更新于2022年05月01日</p>
<hr>
<p>最近忙东忙西，不知道在干嘛。互联网+比赛，项目书、PPT、视频，小论文返修…</p>
<p>实验做了一点点。GAN，生成器，判别器，多层感知机 换成RNN。… </p>
<p>更新于2022年05月26日</p>
<hr>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2022-04-11T16:00:00.000Z" title="4/12/2022, 12:00:00 AM">2022-04-12</time>发表</span><span class="level-item"><time dateTime="2022-06-22T03:43:30.065Z" title="6/22/2022, 11:43:30 AM">2022-06-22</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/GAN/">GAN</a></span><span class="level-item">2 分钟读完 (大约282个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/04/12/SeqGAN/">SeqGAN：具有Policy梯度的序列生成对抗网络</a></h1><div class="content"><h2 id="Publish"><a href="#Publish" class="headerlink" title="Publish"></a>Publish</h2><p>AAAI-2017</p>
<h2 id="title"><a href="#title" class="headerlink" title="title"></a>title</h2><p>SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient</p>
<h2 id="solution-problem"><a href="#solution-problem" class="headerlink" title="solution problem"></a>solution problem</h2><p>生成对抗网络（GAN）在生成real_value数据取得了巨大的成功,但是在用于生成离散数据具有局限性，主要原因在于：来自生成模型的离散输出使其难以从判别模型的梯度更新传递给生成模型。此外：判别模型只能评估完整的序列，而对于部分生成的序列，一旦生成完整序列，就需要去平衡当前和未来的评分。</p>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><ol>
<li>提出一种训练生成模型的新方法SeqGAN  </li>
<li>其数据生成器建模使用强化学习中的随机策略，其中RL奖励值来自GAN判别器对完整序列的评判。使用蒙特卡洛搜索传回中间状态。<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2>在合成数据和现实任务上进行的大量实验表明，与强大的基线相比，有了显著的改进.<h1 id="Other"><a href="#Other" class="headerlink" title="Other"></a>Other</h1><h2 id="蒙特卡洛搜索"><a href="#蒙特卡洛搜索" class="headerlink" title="蒙特卡洛搜索"></a>蒙特卡洛搜索</h2>学习资料<br><a target="_blank" rel="noopener" href="https://blog.csdn.net/luomin2523/article/details/118109154">1</a><br><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_16137569/article/details/83543641">2</a></li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2022-04-07T16:00:00.000Z" title="4/8/2022, 12:00:00 AM">2022-04-08</time>发表</span><span class="level-item"><time dateTime="2022-06-22T03:43:39.567Z" title="6/22/2022, 11:43:39 AM">2022-06-22</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/BERT/">BERT</a></span><span class="level-item">1 分钟读完 (大约150个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/04/08/WhiteningBERT/">WhiteningBERT：降低向量维度且效果提高</a></h1><div class="content"><h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>使用BERT等模型生成句向量，使用FAISS或ElasticSearch（ES）等引擎进行语义向量检索，是工业界常用的方法。然后在巨大的数据量时搜索时间成本巨大。</p>
<p>Whitening-BERT借鉴PCA方法对语义向量进行降维。有效提升语义相似度计算的效果。</p>
<p>首先介绍一下PCA降维降维方法。</p>
<h2 id="PCA（主成分分析）"><a href="#PCA（主成分分析）" class="headerlink" title="PCA（主成分分析）"></a>PCA（主成分分析）</h2><p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1E5411E71z?share_source=copy_web">用最直观的方式告诉你：什么是主成分分析PCA</a></p>
</div></article></div><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous is-invisible is-hidden-mobile"><a href="/page/0/">上一页</a></div><div class="pagination-next"><a href="/page/2/">下一页</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link is-current" href="/">1</a></li><li><a class="pagination-link" href="/page/2/">2</a></li></ul></nav></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/avatar.png" alt="都灵的夏天"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">都灵的夏天</p><p class="is-size-6 is-block">天道酬勤</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>ChongQing</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">11</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">8</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">13</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/ppoffice" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/ppoffice"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">链接</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://hexo.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Hexo</span></span><span class="level-right"><span class="level-item tag">hexo.io</span></span></a></li><li><a class="level is-mobile" href="https://bulma.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Bulma</span></span><span class="level-right"><span class="level-item tag">bulma.io</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/BERT/"><span class="level-start"><span class="level-item">BERT</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/DA/"><span class="level-start"><span class="level-item">DA</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/GAN/"><span class="level-start"><span class="level-item">GAN</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/code/"><span class="level-start"><span class="level-item">code</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%AD%A6%E6%9C%AF%E5%9E%83%E5%9C%BE%E5%88%B6%E9%80%A0/"><span class="level-start"><span class="level-item">学术垃圾制造</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA/"><span class="level-start"><span class="level-item">数据增强</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%9C%AD%E8%AE%B0/"><span class="level-start"><span class="level-item">札记</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E7%A0%94%E7%A9%B6%E7%82%B9/"><span class="level-start"><span class="level-item">研究点</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">最新文章</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-07-02T16:00:00.000Z">2022-07-03</time></p><p class="title"><a href="/2022/07/03/%E7%AC%AC%E4%B8%80%E6%AC%A1%E8%AE%BA%E6%96%87%E6%8A%95%E9%80%92%E5%A4%B1%E8%B4%A5%E7%BB%8F%E5%8E%86/">第一次论文失败经历</a></p><p class="categories"><a href="/categories/%E5%AD%A6%E6%9C%AF%E5%9E%83%E5%9C%BE%E5%88%B6%E9%80%A0/">学术垃圾制造</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-06-21T16:00:00.000Z">2022-06-22</time></p><p class="title"><a href="/2022/06/22/%E6%8D%A2%E4%B8%BB%E9%A2%98%E5%90%8E%E7%AC%AC%E4%B8%80%E7%AF%87/">换了新主题，显示不了LaTeX公式</a></p><p class="categories"><a href="/categories/%E6%9C%AD%E8%AE%B0/">札记</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-06-15T16:00:00.000Z">2022-06-16</time></p><p class="title"><a href="/2022/06/16/%E3%80%8Cpytorch%E3%80%8D%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B/">样例-快速入门Pytorch</a></p><p class="categories"><a href="/categories/code/">code</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-06-13T16:00:00.000Z">2022-06-14</time></p><p class="title"><a href="/2022/06/14/%E3%80%8C%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E3%80%8D2022NAAC%EF%BC%9ATreeMix/">TreeMix：基于组合的数据增强方法</a></p><p class="categories"><a href="/categories/DA/">DA</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-05-26T16:00:00.000Z">2022-05-27</time></p><p class="title"><a href="/2022/05/27/%E3%80%8CGAN%E3%80%8D2021EMNLP%EF%BC%9A%E5%AF%B9LGAN%E8%BF%9B%E8%A1%8CCounter-Contrastive%E5%AD%A6%E4%B9%A0/">Counter-Contrastive 学习：训练Language GAN</a></p><p class="categories"><a href="/categories/GAN/">GAN</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">归档</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2022/07/"><span class="level-start"><span class="level-item">七月 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/06/"><span class="level-start"><span class="level-item">六月 2022</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/05/"><span class="level-start"><span class="level-item">五月 2022</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/04/"><span class="level-start"><span class="level-item">四月 2022</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">标签</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/BERT/"><span class="tag">BERT</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DA/"><span class="tag">DA</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/GAN/"><span class="tag">GAN</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/MixUP/"><span class="tag">MixUP</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/MixUp/"><span class="tag">MixUp</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Mixup/"><span class="tag">Mixup</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/NLP/"><span class="tag">NLP</span><span class="tag">7</span></a></div><div class="control"><a class="tags has-addons" href="/tags/pytorch/"><span class="tag">pytorch</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/"><span class="tag">对比学习</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA/"><span class="tag">数据增强</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%AF%AD%E4%B9%89%E7%9B%B8%E4%BC%BC%E5%BA%A6%E8%AE%A1%E7%AE%97/"><span class="tag">语义相似度计算</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%97%B2%E8%A8%80%E7%A2%8E%E8%AF%AD/"><span class="tag">闲言碎语</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%99%8D%E7%BB%B4/"><span class="tag">降维</span><span class="tag">1</span></a></div></div></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">订阅更新</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="订阅"></div></div></form></div></div></div><div class="card widget"><div class="card-content"><div class="notification is-danger">You need to set <code>client_id</code> and <code>slot_id</code> to show this AD unit. Please set it in <code>_config.yml</code>.</div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">follow.it</h3><form action="" method="post" target="_blank"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="订阅"></div></div></form></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="都灵的夏天" height="28"></a><p class="is-size-7"><span>&copy; 2022 周江峰</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>