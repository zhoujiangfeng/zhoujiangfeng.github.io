{"pages":[],"posts":[{"title":"","text":"SimCSE：一种对比学习数据增强方法TitleSimCSE: Simple Contrastive Learning of Sentence Embeddings Time2021.9 PublishEmnlp2021 Summary 提出了目前在sentence embedding 领域表现最优的对比学习框架SimCSE 首先使用无监督方法，将输入sentence和其预测的sentence1作为对比目标，其只经过一个标准的dropout作为噪音，取得的很好效果，与以前的有监督效果相当。 其发现利用 dropout作为数据增强操作有很好的效果，且移除它会导致表示效果变差。 进而提出有监督方法。标注的自然语言推理数据集输入对比学习框架，使用“entailment”数据作为正样本，“contradiction”作为hard 负样本。 最后在STS任务评测，无监督、有监督simcse+bert_base 斯皮尔曼相关系数分别提升4.2%、2.2% 证明：对比学习 正则化了 预训练模型嵌入 各向异性空间变得更加统一，使用监督学习能够使得正样本更好的对齐。 Method(s)无监督SimCSE实现 将句子输入 dropout作为noise 将同一句输入预训练模型编码器两次：应用标准的dropout两次。获得两个不同的嵌入作为“positive pairs” 同一个batch其他句子，组合作为“negatives” 最后模型从这些“negatives”中预测“positive” train objective同样的input到编码器两次，获得两个embedding$z$,$z^{\\prime}$ $\\ell_{i}=-\\log \\frac{e^{\\operatorname{sim}\\left(\\mathbf{h}{i}^{z{i}}, \\mathbf{h}{i}^{z{i}^{\\prime}}\\right) / \\tau}}{\\sum_{j=1}^{N} e^{\\operatorname{sim}\\left(\\mathbf{h}{i}^{z{i}}, \\mathbf{h}{j}^{z{j}^{\\prime}}\\right) / \\tau}}$ $z$仅仅使用transformer标准的dropout mask 结论：dropout对隐藏的表示起到了最小的“数据扩充”作用，而删除它则会导致表示崩溃。 有监督SimCSE实现 使用自然语言推理数据集，样例如上 1.将原有三分类任务（蕴含，中立，相反）降低为（蕴含，相反），其中“中立”被视为正样本。 2.添加对应的相反 语句对视为 hard negative 进一步提升表现。NLI数据集效果尤其好。 train objective将$x，x_i^+$扩展为$x，x_i^+，x_i^-$,以$x$为基础，$x_i^+$和$x_i^-$为蕴含和相反 假设 其训练目标为 ​ $-\\log \\frac{e^{\\operatorname{sim}\\left(\\mathbf{h}{i}, \\mathbf{h}{i}^{+}\\right) / \\tau}}{\\sum_{j=1}^{N}\\left(e^{\\operatorname{sim}\\left(\\mathbf{h}{i}, \\mathbf{h}{j}^{+}\\right) / \\tau}+e^{\\operatorname{sim}\\left(\\mathbf{h}{i}, \\mathbf{h}{j}^{-}\\right) / \\tau}\\right)}$ Evaluation对比学习分析工具： alignment 测试语义相关的正样本对齐程度 $\\ell_{\\text {align }} \\triangleq \\underset{\\left(x, x^{+}\\right) \\sim p_{\\text {pos }}}{\\mathbb{E}}\\left|f(x)-f\\left(x^{+}\\right)\\right|^{2}$ 其中$p_{pos}$表示 语句对对应的分布式的句向量，假设其中表示均已正则化 uniformity 利用整个表示空间的均匀性来衡量学习嵌入的质量 ​ $\\ell_{\\text {uniform }} \\triangleq \\log \\underset{x, y \\stackrel{\\mathbb{i} . i . d .}{\\sim} p_{\\text {data }}}{ } e^{-2|f(x)-f(y)|^{2}}$ 这两个指标与对比学习的目标是一致的:正面实例应该保持接近，而随机实例的嵌入应该分散在超球体上。 其数字越小越好 Conclusion\u0001 Notes对比学习：对比学习的目的是通过把语义上相近的邻居拉到一起，把非邻居推开来学习有效的表征 假定语句集$\\mathcal{D}=\\left{\\left(x_{i}, x_{i}^{+}\\right)\\right}{i=1}^{m}$，其$x{i}$和$x_{i}^{+}$语义相关 采用交叉熵损失 其$h_{i}$和$h_{i}^{+}$表示$x_{i}$和$x_{i}^{+}$ 对于（$x_{i}$，$x_{i}^{+}$）的Npair mini-batch 的训练目标为 $\\ell_{i}=-\\log \\frac{e^{\\operatorname{sim}\\left(\\mathbf{h}{i}, \\mathbf{h}{i}^{+}\\right) / \\tau}}{\\sum_{j=1}^{N} e^{\\operatorname{sim}\\left(\\mathbf{h}{i}, \\mathbf{h}{j}^{+}\\right) / \\tau}}$ 其中$\\tau$为超参数，$\\operatorname{sim}\\left(\\mathbf{h}{i}, \\mathbf{h}{i}^{+}\\right)$表示cos相似度$\\frac{\\mathbf{h}{1}^{\\top} \\mathbf{h}{2}}{\\left|\\mathbf{h}{1}\\right| \\cdot\\left|\\mathbf{h}{2}\\right|}$ 使用步骤 首先利用PLM编码输入sentence 然后利用上述对比学习目标fine-tune所有参数","link":"/2020/12/26/2022-04-01-simcse%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"title":"","text":"SeqGAN：具有Policy梯度的序列生成对抗网络PublishAAAI-2017 titleSeqGAN: Sequence Generative Adversarial Nets with Policy Gradient solution problem生成对抗网络（GAN）在生成real_value数据取得了巨大的成功,但是在用于生成离散数据具有局限性，主要原因在于：来自生成模型的离散输出使其难以从判别模型的梯度更新传递给生成模型。此外：判别模型只能评估完整的序列，而对于部分生成的序列，一旦生成完整序列，就需要去平衡当前和未来的评分。 Summary 提出一种训练生成模型的新方法SeqGAN 其数据生成器建模使用强化学习中的随机策略，其中RL奖励值来自GAN判别器对完整序列的评判。使用蒙特卡洛搜索传回中间状态。Conclusion在合成数据和现实任务上进行的大量实验表明，与强大的基线相比，有了显著的改进.Other蒙特卡洛搜索学习资料12","link":"/2022/04/12/2022-04-12-SeqGAN/"},{"title":"","text":"Text Smoothing：一种数据结合mix-up的数据增强方法Publish2022ACL titleText Smoothing: Enhance Various Data Augmentation Methods on Text Classification Tasks solution problem进入神经网络之前，标记通常被转换为对应的One-hot表示，这是词汇表的离散分布。平滑表示是从预先训练的MLM中获得候选token的概率，它可以被视为对onr-hot表示的更多信息的替代。我们提出了一种有效的数据增强方法，称为文本平滑，通过将句子从其one-hot表示转换为可控的平滑表示。我们在低资源条件下对不同基准的文本平滑进行了评估。实验结果表明，文本平滑方法的性能明显优于各种主流数据增强方法。此外，文本平滑可以与这些数据增强方法相结合，以获得更好的性能。 文本平滑代码：Pytorch 1234567sentence = &quot;My favorite fruit is pear .&quot;lambd = 0.1 # interpolation hyperparametermlm.train() # enable dropout, dynamically masktensor_input = tokenizer(sentence, return_tensors=&quot;pt&quot;)onehot_repr = convert_to_onehot(**tensor_input)smoothed_repr = softmax(mlm(**tensor_input).logits[0])interpolated_repr = lambd * onehot_repr + (1 - lambd) * smoothed_repr 使用BERT作为MLM，给定下游数据集命名为：$D={t_i,p_i,s_i,l_i}_{i=1}^{N}$ ,N表示样本数量，$t_i$表示文本one-hot 编码，$p_i$表示$t_i$位置编码，$s_i$表示$t_i$的段编码，$l_i$表示实例标签。 将$t_i,p_i,s_i$送入BERT 取回BERT中Transformer-encoder最后一层的输出表示为$$\\overrightarrow{t_i}=BERT(t_i)$$其中$\\overrightarrow{t_i}$形状为[seq_len,emb_size] 然后乘以$\\overrightarrow{t_i}$乘以BERT中词嵌入矩阵$W$,其形状为[vocab_size,embed_size]$$MLM(t_i)=softmax(\\overrightarrow(t_i)W^T)$$其中$MLM(t_i)$中每一行是token词汇表中的概率分布，表示了预训练BERT学习到输入文本所在位置的包含上下文 的标记选项（信息）。 mixup定义为$$\\tilde{x}=\\lambda x_{i}+(1-\\lambda) x_{j}$$ $$\\tilde{y}=\\lambda y_{i}+(1-\\lambda) y_{j}$$ 其中$(x_i,x_j),(y_i,y_j)$为从训练数据中随机抽出两个目标特征向量,$\\lambda\\in[0,1]$在文本平滑中，One-hot表示和平滑表示来自相同的原始输入，标签相同，其内部插入操作不会改变标签，因此mixup操作可以简化为$$\\widetilde{t_{i}}=\\lambda \\cdot t_{i}+(1-\\lambda) \\cdot \\operatorname{MLM}\\left(t_{i}\\right)$$其中$t_i$为one-hot表示，$MLM(t_i)$为平滑表示，$\\widetilde{t_i}$为联合插入表示，$\\lambda$为用于控制插入的超参数。下游任务中我们使用联合表示代替one-hot变化表示作为输入。 Experiment 数据集 Baseline approaches 简介 EDA 由四个简单操作组成：同义词替换、随机插入、随机交换和随机删除。 Back Translation 将句子翻译成临时语言(EN-DE)，然后将先前翻译的文本翻译回源语言(DE-EN) CBERT 用预先训练的BERT mask一些标记并预测它们的上下文替换。 BERTexpand, BERTprepend 通过在给定类的所有示例中添加类标签来满足BERT条件。“expand”标签以 模拟 词汇表，而“prepend”则没有 GPT2context 为预先训练的GPT模型提供提示，并持续生成，直到[EOS]token BARTword, BARTspan 通过在给定类的所有示例前添加类标签来为条件BART。BARTword屏蔽了单个单词，而BARTspan屏蔽了连续的区块。 结果 Conclusion小数据，可控，目前优于其他，未来，结合其他DA=顶会。我也想发顶会啊。","link":"/2022/04/12/2022-05-26-%E3%80%8C%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E3%80%8D2022NAACL%EF%BC%9ATextSmoth/"},{"title":"","text":"Counter-Contrastive 学习：训练Language GANPublish2021EMNLP titleCounter-Contrastive Learning for Language GANs 摘要生成对抗网络(GANs)在图像合成方面取得了巨大的成功，但在生成自然语言方面存在一定的困难。挑战来自于鉴别器传递的uninformative learning signal。换句话说，糟糕的learning singnal限制了生成结构丰富、语义丰富的语言的学习能力。在本文中，我们提出在语言gan中采用反对比学习(CCL)方法来支持生成器的训练。与标准的GANs采用简单的二元分类器来区分样本的真假相比，我们采用了一种反对比学习信号，通过提高语言合成器的训练 (1)把生成样本与真实样本 放在一起（以生成真实的数据） (2)推开真实的样本（以阻碍鉴别的训练）从而防止鉴别器被过度训练。 我们在合成基准和实际基准上评估了我们的方法，并与以前的GAN相比，在对抗序列生成方面产生了具有竞争力的性能。 Solution problemCCL$$\\mathcal{L}{i}=-\\log \\frac{e^{\\operatorname{sim}\\left(\\mathbf{h}{i}, \\mathbf{h}{i}^{-}\\right) / \\tau}}{\\sum{j=1}^{N}\\left(e^{\\operatorname{sim}\\left(\\mathbf{h}{j}, \\mathbf{h}{j}^{-}\\right) / \\tau}+e^{\\operatorname{sim}\\left(\\mathbf{h}{j}, \\mathbf{h}{j}^{+}\\right) / \\tau}\\right)}$$ ExperimentConclusion没代码，不看了。","link":"/2022/04/12/2022-05-27-%E3%80%8CGAN%E3%80%8D2021EMNLP%EF%BC%9A%E5%AF%B9LGAN%E8%BF%9B%E8%A1%8CCounter-Contrastive%E5%AD%A6%E4%B9%A0/"},{"title":"","text":"WhiteningBERT：降低向量维度且效果提高背景使用BERT等模型生成句向量，使用FAISS或ElasticSearch（ES）等引擎进行语义向量检索，是工业界常用的方法。然后在巨大的数据量时搜索时间成本巨大。 Whitening-BERT借鉴PCA方法对语义向量进行降维。有效提升语义相似度计算的效果。 首先介绍一下PCA降维降维方法。 PCA（主成分分析）用最直观的方式告诉你：什么是主成分分析PCA","link":"/2020/12/26/2022-04-08-WhiteningBERT/"},{"title":"","text":"Text Smoothing：一种数据结合mix-up的数据增强方法Publish2022ACL titleSolution problem进入神经网络之前，标记通常被转换为对应的One-hot表示，这是词汇表的离散分布。平滑表示是从预先训练的MLM中获得候选token的概率，它可以被视为对onr-hot表示的更多信息的替代。我们提出了一种有效的数据增强方法，称为文本平滑，通过将句子从其one-hot表示转换为可控的平滑表示。我们在低资源条件下对不同基准的文本平滑进行了评估。实验结果表明，文本平滑方法的性能明显优于各种主流数据增强方法。此外，文本平滑可以与这些数据增强方法相结合，以获得更好的性能。 文本平滑代码：Pytorch 1234567sentence = &quot;My favorite fruit is pear .&quot;lambd = 0.1 # interpolation hyperparametermlm.train() # enable dropout, dynamically masktensor_input = tokenizer(sentence, return_tensors=&quot;pt&quot;)onehot_repr = convert_to_onehot(**tensor_input)smoothed_repr = softmax(mlm(**tensor_input).logits[0])interpolated_repr = lambd * onehot_repr + (1 - lambd) * smoothed_repr 使用BERT作为MLM，给定下游数据集命名为：$D={t_i,p_i,s_i,l_i}_{i=1}^{N}$ ,N表示样本数量，$t_i$表示文本one-hot 编码，$p_i$表示$t_i$位置编码，$s_i$表示$t_i$的段编码，$l_i$表示实例标签。 将$t_i,p_i,s_i$送入BERT 取回BERT中Transformer-encoder最后一层的输出表示为$$\\overrightarrow{t_i}=BERT(t_i)$$其中$\\overrightarrow{t_i}$形状为[seq_len,emb_size] 然后乘以$\\overrightarrow{t_i}$乘以BERT中词嵌入矩阵$W$,其形状为[vocab_size,embed_size]$$MLM(t_i)=softmax(\\overrightarrow(t_i)W^T)$$其中$MLM(t_i)$中每一行是token词汇表中的概率分布，表示了预训练BERT学习到输入文本所在位置的包含上下文 的标记选项（信息）。 mixup定义为$$\\tilde{x}=\\lambda x_{i}+(1-\\lambda) x_{j}$$ $$\\tilde{y}=\\lambda y_{i}+(1-\\lambda) y_{j}$$ 其中$(x_i,x_j),(y_i,y_j)$为从训练数据中随机抽出两个目标特征向量,$\\lambda\\in[0,1]$在文本平滑中，One-hot表示和平滑表示来自相同的原始输入，标签相同，其内部插入操作不会改变标签，因此mixup操作可以简化为$$\\widetilde{t_{i}}=\\lambda \\cdot t_{i}+(1-\\lambda) \\cdot \\operatorname{MLM}\\left(t_{i}\\right)$$其中$t_i$为one-hot表示，$MLM(t_i)$为平滑表示，$\\widetilde{t_i}$为联合插入表示，$\\lambda$为用于控制插入的超参数。下游任务中我们使用联合表示代替one-hot变化表示作为输入。 Experiment 数据集 Baseline approaches 简介 EDA 由四个简单操作组成：同义词替换、随机插入、随机交换和随机删除。 Back Translation 将句子翻译成临时语言(EN-DE)，然后将先前翻译的文本翻译回源语言(DE-EN) CBERT 用预先训练的BERT mask一些标记并预测它们的上下文替换。 BERTexpand, BERTprepend 通过在给定类的所有示例中添加类标签来满足BERT条件。“expand”标签以 模拟 词汇表，而“prepend”则没有 GPT2context 为预先训练的GPT模型提供提示，并持续生成，直到[EOS]token BARTword, BARTspan 通过在给定类的所有示例前添加类标签来为条件BART。BARTword屏蔽了单个单词，而BARTspan屏蔽了连续的区块。 结果 Conclusion小数据，可控，优于其他，未来，结合其他DA=顶会。我也想发顶会啊。","link":"/2022/04/12/2022-05-28-%E3%80%8C%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E3%80%8D2022ACL%EF%BC%9AGlitter/"},{"title":"","text":"TreeMix：基于组合的数据增强方法PublishNAACL2022 TitleTreeMix: Compositional Constituency-based Data Augmentation for Natural Language Understanding Abstract数据增强是解决过度拟合问题的一种有效方法。前人针对自然语言处理提出了不同的数据扩充策略，如噪声注入、单词替换、回译等。虽然有效，但它们忽略了语言的一个重要特征-组合性，复杂表达的意义是从其子部分建立起来的。受此启发，我们提出了一种用于自然语言理解的成分数据扩充方法TreeMix。具体地说，TreeMix利用选区分析树将句子分解成构成子结构，并利用Mixup数据增强技术对它们进行重组以生成新的句子。与以前的方法相比，TreeMix在生成的样本中引入了更大的多样性，并鼓励模型学习NLP数据的组合性。在文本分类和SCAN的大量实验表明，TreeMix的性能优于目前最先进的数据增强方法。 Solution problem合成性是语言的一个关键方面，因为复句的意义是从它的子部分建立起来的。先前的工作还表明，语法树(例如，基于树的LSTM)有助于对句子结构进行建模，以便更好地进行文本分类。然而，在语言技术社区中，除了在语义分析方面的一些例外情况外，利用组合结构进行数据扩充并没有受到太多关注 我们提出了一种用于自然语言理解的成分数据增强方法，即TreeMix(图1)。TreeMix是一种输入级混合方法，它利用成分分析信息，将来自不同句子的不同片段(子树的短语)进行重组，以创建训练集中从未见过的新示例；同时还将基于这些片段策略性地创建新的软标签。这样，TreeMix不仅利用了构成语言的特征来增加扩充的多样性，而且为这些混合的例子提供了合理的软标签。 mixup定义为$$\\tilde{x}=\\lambda x_{i}+(1-\\lambda) x_{j}$$ $$\\tilde{y}=\\lambda y_{i}+(1-\\lambda) y_{j}$$ 其中$(x_i,x_j),(y_i,y_j)$为从训练数据中随机抽出两个目标特征向量,$\\lambda\\in[0,1]$ 我们通过融入语言的组合性来改进Mixup，这是泛化所必需的一个关键特征，但神经模型往往无法捕捉到这一点。我们新提出的方法TreeMix不是用整个样本进行内插，而是通过删除句子的短语并重新插入其他句子的子部分来创建新句子。TreeMix利用选民树将句子分解成有意义的组成部分，然后将这些组成部分移除并重新组合，以生成新的扩充样本。我们的目标是通过对TreeMix生成的大量样本进行训练来提高模型的组合性泛化能力。一个使用TreeMix进行单句分类的例子如上图所示。 TreeMix 详细过程${x}{i}=\\left{x{i}^{1}, x_{i}^{2}, \\ldots, x_{i}^{l}\\right}$表示长度为$l$的序列，其对应的one-hot编码label为$y_i$,我们在$x_i$上运行一个解析器得到它的解析树$T(x_i)$,为了获取序列中有意义的子部分，采用递归遍历解析树，获得所有具有一个以上child的子树。表示子树的集合为$S(x_i)= {t_i^k}$.其中$t_i^k$表示样本$x_i$的第k个子树，对于子树$t_i^k$连续覆盖了$x_i$的$t_{i}^{k} \\triangleq\\left[x_{i}^{r_{k}}, \\ldots, x_{i}^{s_{k}}\\right]$,索引$r_k$为开始，$s_k$为结束。例如图一左侧所示，例句子树可以cover span 的有1.this poor film,2. in this poor film, 3.no interest …etc 对于给定的样本$(x_i,y_i)$，我们从训练集中随机抽取另一个数据点$(x_j, y_j)$。我们对这两个句子运行选区解析器，得到它们的子树集$S(x_i)$和$S(x_j)$，我们可以对要交换的子树进行采样。我们引入两个额外的超参数$λ_L$和$λ_U$来约束待采样子树的长度。$λ_L$和$λ_U$，用子树与原始句子的长度之比来衡量要采样子树的上下限。直观地说，$λ$控制着我们想要交换的短语的粒度。我们希望交换的长度是合理的。如果它太短，那么交换不能给增强样本引入足够的多样性;否则，如果太长，这个过程可能会给原句注入太多噪音。我们设置λ为比率，以便与原句子的长度不变。表2显示了一些具有不同长度约束的子树示例。我们将长度受限子树集合定义为: $S_{\\lambda}(\\mathbf{x}) \\triangleq\\left{t \\mid t \\in S(\\mathbf{x})\\right., s.t. \\left.\\frac{|t|}{|\\mathbf{x}|} \\in\\left[\\lambda_{L}, \\lambda_{U}\\right]\\right}$ 其中$|·|$表示序列或子树的长度，对于两个句子$x_i$和$x_j$,我们随机采样两个子树$t_{i}^{k} \\in S_{\\lambda}\\left(\\mathbf{x}{i}\\right)$和$t{j}^{l} \\in S_{\\lambda}\\left(\\mathbf{x}{j}\\right)$并且通过$t_j^l$替换$t_i^k$构建新的样本。例如$$\\overline{\\mathbf{x}} \\triangleq[x{i}^{1}, \\ldots, x_{i}^{r_{k}-1}, \\underbrace{x_{j}^{r_{l}}, \\ldots, x_{j}^{s_{l}}}{t{j}^{l}}, x_{i}^{s_{k}+1}, \\ldots x_{i}^{l}]$$其中$t_{j}^{l}=\\left[x_{j}^{r_{l}}, \\ldots, x_{j}^{s_{l}}\\right]$替换$t_{i}^{k}=$ $\\left[x_{i}^{r_{k}}, \\ldots, x_{i}^{s_{k}}\\right]$如上图1所示a touching transcend love story 替换this poor film. TreeMix制作标签为扩充的样本$\\overline{x}$创建有效标签是一个具有挑战性的问题。类似于Mixup，我们使用原始的凸组合两个句子的标签作为扩充样本的标签。$$\\overline{\\mathbf{y}}=\\frac{l_{i}-\\left|t_{i}^{k}\\right|}{l_{i}-\\left|t_{i}^{k}\\right|+\\left|t_{j}^{l}\\right|} \\mathbf{y}{i}+\\frac{\\left|t{j}^{l}\\right|}{l_{i}-\\left|t_{i}^{k}\\right|+\\left|t_{j}^{l}\\right|} \\mathbf{y}_{j}$$其中$l_i$为$x_i$的长度，$|t_i^k|$为子树的长度，在新的句子中,从$x_i$中保留$l_i-|t_i^k|$个words，从句子$x_j$插入$|t_j^l|$个words。 $\\frac{l_{i}-\\left|t_{i}^{k}\\right|}{l_{i}-\\left|t_{i}^{k}\\right|+\\left|t_{j}^{l}\\right|}$是来自$x_i$的words的分数，其可以决定$y_i$的权重，然后，基于标签的变化与原始句子中的长度变化成比例的猜想来创建标签。附录提供样本。 组合算法 我们的主要算法如算法1所示。虽然TreeMix创建的句子并不都是流畅的甚至有效的新句子，但它们包含具有不同含义的子部分，这鼓励模型以组合的方式构建丰富的句子表示。需要注意的是，扩展后的标签是原始标签的凸组合，只有当模型学习到两个部分的表示在一起时，它们才能预测具有不同权重的两个标签。 Training Objective我们的模型是在原始样本和增强样本的组合上训练，以获得正则化和噪声注入之间的权衡。最终的培训目标是:$$\\begin{aligned}\\mathcal{L}=&amp; \\underset{(\\mathbf{x}, \\mathbf{y}) \\sim D}{\\mathbb{E}}\\left[-\\mathbf{y}^{\\top} \\log P_{\\theta}(\\mathbf{y} \\mid \\mathbf{x})\\right]+\\gamma \\underset{(\\overline{\\mathbf{x}}, \\overline{\\mathbf{y}}) \\sim D^{\\prime}}{\\mathbb{E}}\\left[-\\overline{\\mathbf{y}}^{\\top} \\log P_{\\theta}(\\overline{\\mathbf{y}} \\mid \\overline{\\mathbf{x}})\\right]\\end{aligned}$$$\\gamma$i是增强样本的权重 Experiment 数据集 Baseline approaches 简介 BERT EDA 由四个简单操作组成：同义词替换、随机插入、随机交换和随机删除。 AEDA 在文本中随机插入标点符号的AEDA Back Translation 将句子翻译成临时语言(EN-DE)，然后将先前翻译的文本翻译回源语言(DE-EN) GPT3Mix 设计提示并利用GPT3生成新的示例来训练模型。 SSMix 通过在给定类的所有示例前添加类标签来为条件BART。BARTword屏蔽了单个单词，而BARTspan屏蔽了连续的区块。 EmbedMix Tmix 首先对两个输入分别编码，然后在某一编码器层a处对两个嵌入进行线性插值，最终向前传递组合嵌入到其余层中。 结果 Conclusion没找到代码，自己摸索了。","link":"/2022/04/12/2022-06-14-%E3%80%8C%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E3%80%8D2022NAAC%EF%BC%9ATreeMix/"},{"title":"","text":"样例-快速入门Pytorch1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465import torchimport torch.nn as nnimport torch.nn.functional as Fimport numpy as npfrom torch.autograd import Variablefrom sklearn.model_selection import train_test_splitimport torch.optim as optimclass modellogit(nn.Module): def __init__(self): nn.Module.__init__(self) self.fc1 = nn.Linear(2,16) self.fc2 = nn.Linear(16,8) self.fc3 = nn.Linear(8,1) def forward(self,x): x = torch.relu(self.fc1(x)) x = torch.relu(self.fc2(x)) x = self.fc3(x) return x class hello_model(object): def __init__(self,max_iter=200,learning_rate=0.01,l1=1e-4): self.model = modellogit() self.max_iter = max_iter self.learning_rate = learning_rate self.l1 = l1 def fit(self,train_x,train_y): optimizer = optim.Adam(self.model.parameters(),lr = self.learning_rate) criterion = nn.MSELoss() print('训练开始') for epoch in range(self.max_iter): input_x = torch.from_numpy(train_x).float() target = torch.from_numpy(train_y).float() #梯度置零 optimizer.zero_grad() #正向传播 output = self.model(input_x) #反向传播 loss = criterion(output, target) regular_loss = 0 for param in self.model.parameters(): regular_loss += torch.sum(torch.abs(param)) loss += self.l1*regular_loss loss.backward() #优化 optimizer.step() if epoch%int(self.max_iter/5) == 0: print('[%d, %5d] loss: %.3f'%(epoch+1, epoch+1, loss.data)) print('训练结束') def predict(self,test_x): y_hat = self.model(torch.from_numpy(test_x).float()) y_hat = y_hat.detach() y_hat = y_hat.numpy() return(y_hat) data_x = np.random.normal(size = (30000,2))data_y = data_x[:,[0]] + data_x[:,[0]]**2 + data_x[:,[1]]**2+\\ data_x[:,[1]]*4+np.random.normal(size = (30000,1))*0.1train_x,test_x,train_y,test_y = train_test_split(data_x,data_y,test_size=0.33, random_state=42)model = hello_model(max_iter=5000,learning_rate=0.001)model.fit(train_x,train_y)model.predict(test_x)","link":"/2022/04/12/2022-06-16-%E3%80%8Cpytorch%E3%80%8D%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2022/06/21/hello-world/"}],"tags":[],"categories":[]}