<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>都灵的夏天</title>
  
  <subtitle>学习笔记</subtitle>
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2024-07-09T08:20:32.000Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>周江峰</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title></title>
    <link href="http://example.com/2024/07/09/C#%20WebApi%20%E5%BF%AB%E9%80%9F%E9%85%8D%E7%BD%AElog4net%E6%97%A5%E5%BF%97%E8%AE%B0%E5%BD%95/"/>
    <id>http://example.com/2024/07/09/C#%20WebApi%20%E5%BF%AB%E9%80%9F%E9%85%8D%E7%BD%AElog4net%E6%97%A5%E5%BF%97%E8%AE%B0%E5%BD%95/</id>
    <published>2024-07-08T16:00:00.000Z</published>
    <updated>2024-07-09T08:20:32.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="c-webapi-快速配置log4net日志记录">C# WebApi 快速配置log4net日志记录</h1><h2 id="环境">1.环境</h2><ul><li><p>net45</p></li><li><p>swagger</p></li></ul><h1 id="log4net配置">2.log4net配置</h1><ul><li><p>nuget安装log4net</p></li><li><p>项目主目录创建log3net.cs</p></li></ul><figure class="highlight cs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">using</span> log4net;</span><br><span class="line"><span class="keyword">using</span> log4net.Appender;</span><br><span class="line"><span class="keyword">using</span> System;</span><br><span class="line"><span class="keyword">using</span> System.Collections.Generic;</span><br><span class="line"><span class="keyword">using</span> System.IO;</span><br><span class="line"><span class="keyword">using</span> System.Linq;</span><br><span class="line"><span class="keyword">using</span> System.Web;</span><br><span class="line"></span><br><span class="line"><span class="keyword">namespace</span> <span class="title">yourspace</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">class</span> <span class="title">AppLog</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">static</span> <span class="built_in">string</span> filepath = AppDomain.CurrentDomain.BaseDirectory + <span class="string">@&quot;\SysLog\&quot;</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">readonly</span> log4net.ILog logComm = log4net.LogManager.GetLogger(<span class="string">&quot;AppLog&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">static</span> <span class="title">AppLog</span>()</span></span><br><span class="line">        &#123;</span><br><span class="line">            log4net.Config.XmlConfigurator.Configure(<span class="keyword">new</span> FileInfo(<span class="string">&quot;log4net.config&quot;</span>));</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (!Directory.Exists(filepath))</span><br><span class="line">            &#123;</span><br><span class="line">                Directory.CreateDirectory(filepath);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">readonly</span> <span class="built_in">object</span> o = <span class="keyword">new</span> <span class="built_in">object</span>();</span><br><span class="line">        <span class="comment"><span class="doctag">///</span> <span class="doctag">&lt;summary&gt;</span></span></span><br><span class="line">        <span class="comment"><span class="doctag">///</span> 写入日志</span></span><br><span class="line">        <span class="comment"><span class="doctag">///</span> <span class="doctag">&lt;/summary&gt;</span></span></span><br><span class="line">        <span class="comment"><span class="doctag">///</span> <span class="doctag">&lt;param name=&quot;msg&quot;&gt;</span>日志内容<span class="doctag">&lt;/param&gt;</span></span></span><br><span class="line">        <span class="comment"><span class="doctag">///</span> <span class="doctag">&lt;param name=&quot;isWrite&quot;&gt;</span>是否写<span class="doctag">&lt;/param&gt;</span></span></span><br><span class="line">        <span class="comment"><span class="doctag">///</span> <span class="doctag">&lt;param name=&quot;action&quot;&gt;</span>写日志的方法<span class="doctag">&lt;/param&gt;</span></span></span><br><span class="line">        <span class="comment"><span class="doctag">///</span> <span class="doctag">&lt;param name=&quot;info&quot;&gt;</span>日志文件名，便于分开日志文件<span class="doctag">&lt;/param&gt;</span></span></span><br><span class="line">        <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">WriteLog</span>(<span class="params"><span class="built_in">string</span> msg, <span class="built_in">bool</span> isWrite, Action&lt;<span class="built_in">object</span>&gt; action, <span class="built_in">string</span> info = <span class="string">&quot;&quot;</span></span>)</span></span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span> (isWrite)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="keyword">lock</span> (o)</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="built_in">string</span> filename = <span class="string">$&quot;AppLog_<span class="subst">&#123;action.Method.Name&#125;</span>_<span class="subst">&#123;info&#125;</span>_<span class="subst">&#123; DateTime.Now.ToString(<span class="string">&quot;yyyyMMdd_HH&quot;</span>)&#125;</span>.log&quot;</span>;</span><br><span class="line">                    <span class="keyword">var</span> repository = LogManager.GetRepository();</span><br><span class="line"></span><br><span class="line">                    <span class="meta">#<span class="keyword">region</span> MyRegion</span></span><br><span class="line">                    <span class="keyword">var</span> appenders = repository.GetAppenders();</span><br><span class="line">                    <span class="keyword">if</span> (appenders.Length &gt; <span class="number">0</span>)</span><br><span class="line">                    &#123;</span><br><span class="line">                        RollingFileAppender targetApder = <span class="literal">null</span>;</span><br><span class="line">                        <span class="keyword">foreach</span> (<span class="keyword">var</span> Apder <span class="keyword">in</span> appenders)</span><br><span class="line">                        &#123;</span><br><span class="line">                            <span class="keyword">if</span> (Apder.Name == <span class="string">&quot;AppLog&quot;</span>)</span><br><span class="line">                            &#123;</span><br><span class="line">                                targetApder = Apder <span class="keyword">as</span> RollingFileAppender;</span><br><span class="line">                                <span class="keyword">break</span>;</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line">                        <span class="keyword">if</span> (targetApder.Name == <span class="string">&quot;AppLog&quot;</span>)<span class="comment">//如果是文件输出类型日志，则更改输出路径</span></span><br><span class="line">                        &#123;</span><br><span class="line">                            <span class="keyword">if</span> (targetApder != <span class="literal">null</span>)</span><br><span class="line">                            &#123;</span><br><span class="line">                                <span class="keyword">if</span> (!targetApder.File.Contains(filename))</span><br><span class="line">                                &#123;</span><br><span class="line">                                    targetApder.File = <span class="string">@&quot;SysLog\&quot;</span> + filename;</span><br><span class="line">                                    targetApder.ActivateOptions();</span><br><span class="line">                                &#125;</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="meta">#<span class="keyword">endregion</span></span></span><br><span class="line">                    action(msg);</span><br><span class="line">                    <span class="comment">//logComm.Error(msg + &quot;\n&quot;);</span></span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment"><span class="doctag">///</span> <span class="doctag">&lt;summary&gt;</span></span></span><br><span class="line">        <span class="comment"><span class="doctag">///</span> </span></span><br><span class="line">        <span class="comment"><span class="doctag">///</span> <span class="doctag">&lt;/summary&gt;</span></span></span><br><span class="line">        <span class="comment"><span class="doctag">///</span> <span class="doctag">&lt;param name=&quot;msg&quot;&gt;</span>日志内容<span class="doctag">&lt;/param&gt;</span></span></span><br><span class="line">        <span class="comment"><span class="doctag">///</span> <span class="doctag">&lt;param name=&quot;info&quot;&gt;</span>日志文件名，便于分开日志文件<span class="doctag">&lt;/param&gt;</span></span></span><br><span class="line">        <span class="comment"><span class="doctag">///</span> <span class="doctag">&lt;param name=&quot;isWrite&quot;&gt;</span>是否写入<span class="doctag">&lt;/param&gt;</span></span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">WriteError</span>(<span class="params"><span class="built_in">string</span> msg, <span class="built_in">string</span> info = <span class="string">&quot;&quot;</span>, <span class="built_in">bool</span> isWrite = <span class="literal">true</span></span>)</span></span><br><span class="line">        &#123;</span><br><span class="line">            WriteLog(msg, isWrite, logComm.Error, info);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">WriteInfo</span>(<span class="params"><span class="built_in">string</span> msg, <span class="built_in">string</span> info = <span class="string">&quot;&quot;</span>, <span class="built_in">bool</span> isWrite = <span class="literal">true</span></span>)</span></span><br><span class="line">        &#123;</span><br><span class="line">            WriteLog(msg, isWrite, logComm.Info, info);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">WriteWarn</span>(<span class="params"><span class="built_in">string</span> msg, <span class="built_in">string</span> info = <span class="string">&quot;&quot;</span>, <span class="built_in">bool</span> isWrite = <span class="literal">true</span></span>)</span></span><br><span class="line">        &#123;</span><br><span class="line">            WriteLog(msg, isWrite, logComm.Warn, info);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">WriteFatal</span>(<span class="params"><span class="built_in">string</span> msg, <span class="built_in">string</span> info = <span class="string">&quot;&quot;</span>, <span class="built_in">bool</span> isWrite = <span class="literal">true</span></span>)</span></span><br><span class="line">        &#123;</span><br><span class="line">            WriteLog(msg, isWrite, logComm.Fatal, info);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>主目录创建log4net.config</li></ul><figure class="highlight cs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=<span class="string">&quot;1.0&quot;</span> encoding=<span class="string">&quot;utf-8&quot;</span>?&gt;</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;configSections&gt;</span><br><span class="line">        &lt;section name=<span class="string">&quot;log4net&quot;</span></span><br><span class="line">          type=<span class="string">&quot;log4net.Config.Log4NetConfigurationSectionHandler,&amp;#xD;&amp;#xA;log4net-net-1.0&quot;</span>/&gt;</span><br><span class="line">    &lt;/configSections&gt;</span><br><span class="line">    &lt;log4net&gt;</span><br><span class="line">        &lt;appender name=<span class="string">&quot;AppLog&quot;</span> type=<span class="string">&quot;log4net.Appender.RollingFileAppender&quot;</span>&gt;</span><br><span class="line">            &lt;param name=<span class="string">&quot;File&quot;</span> <span class="keyword">value</span>=<span class="string">&quot;SysLog/&quot;</span> /&gt;</span><br><span class="line">            &lt;param name=<span class="string">&quot;AppendToFile&quot;</span> <span class="keyword">value</span>=<span class="string">&quot;true&quot;</span> /&gt;</span><br><span class="line">            &lt;param name=<span class="string">&quot;MaxSizeRollBackups&quot;</span> <span class="keyword">value</span>=<span class="string">&quot;-1&quot;</span> /&gt;</span><br><span class="line">            &lt;!--最小锁定模型以允许多个进程可以写入同一个文件--&gt;</span><br><span class="line">            &lt;param name=<span class="string">&quot;lockingModel&quot;</span>  type=<span class="string">&quot;log4net.Appender.FileAppender+MinimalLock&quot;</span> /&gt;</span><br><span class="line">            &lt;param name=<span class="string">&quot;MaximumFileSize&quot;</span> <span class="keyword">value</span>=<span class="string">&quot;10MB&quot;</span> /&gt;</span><br><span class="line">            &lt;param name=<span class="string">&quot;RollingStyle&quot;</span> <span class="keyword">value</span>=<span class="string">&quot;Size&quot;</span> /&gt;</span><br><span class="line">            &lt;param name=<span class="string">&quot;DatePattern&quot;</span> <span class="keyword">value</span>=<span class="string">&quot;yyyy-MM-dd&quot;</span> /&gt;</span><br><span class="line">            &lt;param name=<span class="string">&quot;StaticLogFileName&quot;</span> <span class="keyword">value</span>=<span class="string">&quot;true&quot;</span> /&gt;</span><br><span class="line">            &lt;layout type=<span class="string">&quot;log4net.Layout.PatternLayout&quot;</span>&gt;</span><br><span class="line">                &lt;param name=<span class="string">&quot;ConversionPattern&quot;</span> <span class="keyword">value</span>=<span class="string">&quot;%-5p %d [%c] %m%n&quot;</span> /&gt;</span><br><span class="line">            &lt;/layout&gt;</span><br><span class="line">        &lt;/appender&gt;</span><br><span class="line">        &lt;logger name=<span class="string">&quot;AppLog&quot;</span>&gt;</span><br><span class="line">            &lt;level <span class="keyword">value</span>=<span class="string">&quot;all&quot;</span> /&gt;</span><br><span class="line">            &lt;appender-<span class="keyword">ref</span> <span class="keyword">ref</span>=<span class="string">&quot;AppLog&quot;</span> /&gt;</span><br><span class="line">        &lt;/logger&gt;</span><br><span class="line">        &lt;root&gt;</span><br><span class="line">            &lt;level <span class="keyword">value</span>=<span class="string">&quot;all&quot;</span> /&gt;</span><br><span class="line">        &lt;/root&gt;</span><br><span class="line">    &lt;/log4net&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><ul><li>修改yourspace.cs最末尾加入</li></ul><figure class="highlight cs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[<span class="meta">assembly: AssemblyVersion(<span class="string">&quot;1.0.0.0&quot;</span>)</span>]</span><br><span class="line">[<span class="meta">assembly: AssemblyFileVersion(<span class="string">&quot;1.0.0.0&quot;</span>)</span>]</span><br><span class="line">[<span class="meta">assembly:log4net.Config.XmlConfigurator(ConfigFile =<span class="string">&quot;log4net.config&quot;</span>,Watch =true)</span>]</span><br></pre></td></tr></table></figure><h2 id="使用方法">3.使用方法</h2><p>在你想要的记录log的地方添加</p><figure class="highlight cs"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">AppLog.WriteInfo(“记录的字符串”, <span class="string">&quot;日志名称&quot;</span>, <span class="literal">true</span>);</span><br></pre></td></tr></table></figure><h2 id="总结">4. 总结</h2><p>会在你的主目录SysLog文件夹生成日志文件</p><p><img src="C:\Users\cyzxqzjf\AppData\Roaming\marktext\images\2024-07-09-10-41-43-image.png" /></p><p><a href="https://www.luofenming.com/show.aspx?id=ART2021051500001">参考文档</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;c-webapi-快速配置log4net日志记录&quot;&gt;C# WebApi 快速配置log4net日志记录&lt;/h1&gt;
&lt;h2 id=&quot;环境&quot;&gt;1.环境&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;net45&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;swagger&lt;/p&gt;&lt;/li&gt;
&lt;/</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>第一次论文失败经历</title>
    <link href="http://example.com/2022/07/03/%E7%AC%AC%E4%B8%80%E6%AC%A1%E8%AE%BA%E6%96%87%E6%8A%95%E9%80%92%E5%A4%B1%E8%B4%A5%E7%BB%8F%E5%8E%86/"/>
    <id>http://example.com/2022/07/03/%E7%AC%AC%E4%B8%80%E6%AC%A1%E8%AE%BA%E6%96%87%E6%8A%95%E9%80%92%E5%A4%B1%E8%B4%A5%E7%BB%8F%E5%8E%86/</id>
    <published>2022-07-02T16:00:00.000Z</published>
    <updated>2022-08-18T08:47:16.490Z</updated>
    
    <content type="html"><![CDATA[<p>去年九月份，采购问答机器人项目由于缺乏数据，原始文件就几百条，导致模型准确率较低，老板催促解决这个问题，在组会上提出了数据扩充技术，下来实验，采用回译尝试了一下，遂进入数据增强方面工作，由于国庆结束，去兵科院实习了三个月，在实习之余完成了 基本实验，开年2-4月继续实验，改论文，4月初在老板的要求下投了一个CCF C类期刊，计算机应用，感觉这个名字还挺好听，当时幻想着一投就中，所以改了又改，图画了又画（全文唯一满意的就是模型图了）。然后7月第一天，下午去实验室，收到邮件，让修改后录增刊，给老板发消息，老板说不投了，再想办法。</p><hr /><p>全过程</p><figure><img src="https://cdn.jsdelivr.net/gh/zhoujiangfeng/images@main/20220703005118.png" alt="投递全过程" /><figcaption aria-hidden="true">投递全过程</figcaption></figure><p>耗费了3个月，6-7月其实光在等结果了，无心第二篇的写作。ε=(´ο｀*)))唉</p><p>本来6月10号三审就结束了，中间直接停滞了半个月。</p><p>希望下次运气好一点。</p><p>近期再写一篇，继续投，缓解一下焦虑。没论文在手里可太难了。都不能全身心投入工作准备中...</p><p>祝好运！加油</p><hr /><h3 id="计算机工程ccf-c">计算机工程CCF-C</h3><figure><img src="https://cdn.jsdelivr.net/gh/zhoujiangfeng/images@main/20220818164423.png" alt="image-20220818164358521" /><figcaption aria-hidden="true">image-20220818164358521</figcaption></figure><hr /><h3 id="计算机应用与软件ccf-c">计算机应用与软件CCF-C</h3><figure><img src="https://cdn.jsdelivr.net/gh/zhoujiangfeng/images@main/20220818164608.png" alt="image-20220818164608343" /><figcaption aria-hidden="true">image-20220818164608343</figcaption></figure><hr /><p>真是个学术小垃圾呀</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;去年九月份，采购问答机器人项目由于缺乏数据，原始文件就几百条，导致模型准确率较低，老板催促解决这个问题，在组会上提出了数据扩充技术，下来实验，采用回译尝试了一下，遂进入数据增强方面工作，由于国庆结束，去兵科院实习了三个月，在实习之余完成了 基本实验，开年2-4月继续实验，改</summary>
      
    
    
    
    <category term="学术垃圾制造" scheme="http://example.com/categories/%E5%AD%A6%E6%9C%AF%E5%9E%83%E5%9C%BE%E5%88%B6%E9%80%A0/"/>
    
    
    <category term="NLP" scheme="http://example.com/tags/NLP/"/>
    
    <category term="DA" scheme="http://example.com/tags/DA/"/>
    
  </entry>
  
  <entry>
    <title>换了新主题，显示不了LaTeX公式</title>
    <link href="http://example.com/2022/06/22/%E6%8D%A2%E4%B8%BB%E9%A2%98%E5%90%8E%E7%AC%AC%E4%B8%80%E7%AF%87/"/>
    <id>http://example.com/2022/06/22/%E6%8D%A2%E4%B8%BB%E9%A2%98%E5%90%8E%E7%AC%AC%E4%B8%80%E7%AF%87/</id>
    <published>2022-06-21T16:00:00.000Z</published>
    <updated>2022-06-22T05:22:25.515Z</updated>
    
    <content type="html"><![CDATA[<p>昨天下午心血来潮，想换一个博客主题，然后捣腾半天，睡前弄好了。</p><p>文章迁移，公式又显示不了，尝试了多种方法，还是没有解决。哎</p><p>难道以后公式只能截图嘛😭</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;昨天下午心血来潮，想换一个博客主题，然后捣腾半天，睡前弄好了。&lt;/p&gt;
&lt;p&gt;文章迁移，公式又显示不了，尝试了多种方法，还是没有解决。哎&lt;/p&gt;
&lt;p&gt;难道以后公式只能截图嘛😭&lt;/p&gt;
</summary>
      
    
    
    
    <category term="札记" scheme="http://example.com/categories/%E6%9C%AD%E8%AE%B0/"/>
    
    
    <category term="闲言碎语" scheme="http://example.com/tags/%E9%97%B2%E8%A8%80%E7%A2%8E%E8%AF%AD/"/>
    
  </entry>
  
  <entry>
    <title>样例-快速入门Pytorch</title>
    <link href="http://example.com/2022/06/16/%E3%80%8Cpytorch%E3%80%8D%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B/"/>
    <id>http://example.com/2022/06/16/%E3%80%8Cpytorch%E3%80%8D%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B/</id>
    <published>2022-06-15T16:00:00.000Z</published>
    <updated>2022-06-22T03:47:57.737Z</updated>
    
    <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">modellogit</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        nn.Module.__init__(self)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">2</span>,<span class="number">16</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">16</span>,<span class="number">8</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">8</span>,<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        x = torch.relu(self.fc1(x))</span><br><span class="line">        x = torch.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">hello_model</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,max_iter=<span class="number">200</span>,learning_rate=<span class="number">0.01</span>,l1=<span class="number">1e-4</span></span>):</span><br><span class="line">        self.model = modellogit()</span><br><span class="line">        self.max_iter = max_iter</span><br><span class="line">        self.learning_rate = learning_rate</span><br><span class="line">        self.l1 = l1</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">fit</span>(<span class="params">self,train_x,train_y</span>):</span><br><span class="line">        optimizer = optim.Adam(self.model.parameters(),lr = self.learning_rate)</span><br><span class="line">        criterion = nn.MSELoss()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;训练开始&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(self.max_iter):</span><br><span class="line">            input_x = torch.from_numpy(train_x).<span class="built_in">float</span>()</span><br><span class="line">            target = torch.from_numpy(train_y).<span class="built_in">float</span>()</span><br><span class="line">            <span class="comment">#梯度置零</span></span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            <span class="comment">#正向传播</span></span><br><span class="line">            output = self.model(input_x)</span><br><span class="line">            <span class="comment">#反向传播</span></span><br><span class="line">            loss = criterion(output, target)</span><br><span class="line">            regular_loss = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> param <span class="keyword">in</span> self.model.parameters():</span><br><span class="line">                regular_loss += torch.<span class="built_in">sum</span>(torch.<span class="built_in">abs</span>(param))</span><br><span class="line">            loss += self.l1*regular_loss</span><br><span class="line">            loss.backward()</span><br><span class="line">            <span class="comment">#优化</span></span><br><span class="line">            optimizer.step()</span><br><span class="line">            <span class="keyword">if</span> epoch%<span class="built_in">int</span>(self.max_iter/<span class="number">5</span>) == <span class="number">0</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&#x27;[%d, %5d] loss: %.3f&#x27;</span>%(epoch+<span class="number">1</span>, epoch+<span class="number">1</span>, loss.data))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;训练结束&#x27;</span>)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">self,test_x</span>):</span><br><span class="line">        y_hat = self.model(torch.from_numpy(test_x).<span class="built_in">float</span>())</span><br><span class="line">        y_hat = y_hat.detach()</span><br><span class="line">        y_hat = y_hat.numpy()</span><br><span class="line">        <span class="keyword">return</span>(y_hat)</span><br><span class="line">    </span><br><span class="line">data_x = np.random.normal(size = (<span class="number">30000</span>,<span class="number">2</span>))</span><br><span class="line">data_y = data_x[:,[<span class="number">0</span>]] + data_x[:,[<span class="number">0</span>]]**<span class="number">2</span> + data_x[:,[<span class="number">1</span>]]**<span class="number">2</span>+\</span><br><span class="line">    data_x[:,[<span class="number">1</span>]]*<span class="number">4</span>+np.random.normal(size = (<span class="number">30000</span>,<span class="number">1</span>))*<span class="number">0.1</span></span><br><span class="line">train_x,test_x,train_y,test_y = train_test_split(data_x,data_y,test_size=<span class="number">0.33</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line">model = hello_model(max_iter=<span class="number">5000</span>,learning_rate=<span class="number">0.001</span>)</span><br><span class="line">model.fit(train_x,train_y)</span><br><span class="line">model.predict(test_x)</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span clas</summary>
      
    
    
    
    <category term="code" scheme="http://example.com/categories/code/"/>
    
    
    <category term="pytorch" scheme="http://example.com/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>TreeMix：基于组合的数据增强方法</title>
    <link href="http://example.com/2022/06/14/%E3%80%8C%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E3%80%8D2022NAAC%EF%BC%9ATreeMix/"/>
    <id>http://example.com/2022/06/14/%E3%80%8C%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E3%80%8D2022NAAC%EF%BC%9ATreeMix/</id>
    <published>2022-06-13T16:00:00.000Z</published>
    <updated>2022-06-22T05:11:00.007Z</updated>
    
    <content type="html"><![CDATA[<h2 id="publish">Publish</h2><p>NAACL2022</p><h2 id="title">Title</h2><p>TreeMix: Compositional Constituency-based Data Augmentation for Natural Language Understanding</p><h2 id="abstract">Abstract</h2><p>数据增强是解决过度拟合问题的一种有效方法。前人针对自然语言处理提出了不同的数据扩充策略，如噪声注入、单词替换、回译等。虽然有效，但它们<strong>忽略了语言的一个重要特征-组合性</strong>，<strong>复杂表达的意义是从其子部分建立起来的</strong>。受此启发，我们提出了一种用于自然语言理解的成分数据扩充方法TreeMix。具体地说，<strong>TreeMix利用选区分析树将句子分解成构成子结构，并利用Mixup数据增强技术对它们进行重组以生成新的句子</strong>。与以前的方法相比，TreeMix在生成的样本中引入了更大的<strong>多样性</strong>，并鼓励模型学习NLP数据的<strong>组合性</strong>。在文本分类和SCAN的大量实验表明，TreeMix的性能优于目前最先进的数据增强方法。</p><h2 id="solution-problem">Solution problem</h2><p>合成性是语言的一个关键方面，因为复句的意义是从它的子部分建立起来的。先前的工作还表明，语法树(例如，基于树的LSTM)有助于对句子结构进行建模，以便更好地进行文本分类。然而，在语言技术社区中，除了在语义分析方面的一些例外情况外，利用组合结构进行数据扩充并没有受到太多关注</p><figure><img src="https://cdn.jsdelivr.net/gh/zhoujiangfeng/images@main/20220613135323.png" alt="image-20220613135315056" /><figcaption aria-hidden="true">image-20220613135315056</figcaption></figure><p>我们提出了一种用于自然语言理解的成分数据增强方法，即TreeMix(图1)。TreeMix是一种输入级混合方法，它利用成分分析信息，将来自不同句子的不同片段(子树的短语)进行重组，以创建训练集中从未见过的新示例；同时还将基于这些片段策略性地创建新的软标签。这样，TreeMix不仅利用了构成语言的特征来增加扩充的多样性，而且为这些混合的例子提供了合理的软标签。</p><p>mixup定义为 <span class="math display">\[\tilde{x}=\lambda x_{i}+(1-\lambda) x_{j}\]</span></p><p><span class="math display">\[\tilde{y}=\lambda y_{i}+(1-\lambda) y_{j}\]</span></p><p>其中<span class="math inline">\((x_i,x_j),(y_i,y_j)\)</span>为从训练数据中随机抽出两个目标特征向量,<span class="math inline">\(\lambda\in[0,1]\)</span></p><p>我们通过融入语言的组合性来改进Mixup，这是泛化所必需的一个关键特征，但神经模型往往无法捕捉到这一点。我们新提出的方法TreeMix不是用整个样本进行内插，而是<strong>通过删除句子的短语并重新插入其他句子的子部分来创建新句子</strong>。TreeMix利用选民树将句子分解成有意义的组成部分，然后将这些组成部分移除并重新组合，以生成新的扩充样本。我们的目标是通过对TreeMix生成的大量样本进行训练来提高模型的组合性泛化能力。一个使用TreeMix进行单句分类的例子如上图所示。</p><h3 id="treemix-详细过程">TreeMix 详细过程</h3><p><span class="math inline">\({x}_{i}=\left\{x_{i}^{1}, x_{i}^{2}, \ldots, x_{i}^{l}\right\}\)</span>表示长度为<span class="math inline">\(l\)</span>的序列，其对应的one-hot编码label为<span class="math inline">\(y_i\)</span>,我们在<span class="math inline">\(x_i\)</span>上运行一个解析器得到它的解析树<span class="math inline">\(T(x_i)\)</span>,为了获取序列中有意义的子部分，采用递归遍历解析树，获得所有具有一个以上child的子树。表示子树的集合为<span class="math inline">\(S(x_i)= \{t_i^k\}\)</span>.其中<span class="math inline">\(t_i^k\)</span>表示样本<span class="math inline">\(x_i\)</span>的第k个子树，对于子树<span class="math inline">\(t_i^k\)</span>连续覆盖了<span class="math inline">\(x_i\)</span>的<span class="math inline">\(t_{i}^{k} \triangleq\left[x_{i}^{r_{k} }, \ldots, x_{i}^{s_{k} }\right]\)</span>,索引<span class="math inline">\(r_k\)</span>为开始，<span class="math inline">\(s_k\)</span>为结束。例如图一左侧所示，例句子树可以cover span 的有1.<em>this poor film</em>,2. <em>in this poor film</em>, 3.<em>no interest ...etc</em></p><p><img src="https://cdn.jsdelivr.net/gh/zhoujiangfeng/images@main/20220614152117.png" style="zoom:50%;" /></p><p>对于给定的样本<span class="math inline">\((x_i,y_i)\)</span>，我们从训练集中随机抽取另一个数据点<span class="math inline">\((x_j, y_j)\)</span>。我们对这两个句子运行选区解析器，得到它们的子树集<span class="math inline">\(S(x_i)\)</span>和<span class="math inline">\(S(x_j)\)</span>，我们可以对要交换的子树进行采样。我们引入两个额外的超参数<span class="math inline">\(λ_L\)</span>和<span class="math inline">\(λ_U\)</span>来约束待采样子树的长度。<span class="math inline">\(λ_L\)</span>和<span class="math inline">\(λ_U\)</span>，用子树与原始句子的长度之比来衡量要采样子树的上下限。直观地说，<span class="math inline">\(λ\)</span>控制着我们想要交换的短语的粒度。我们希望交换的长度是合理的。如果它太短，那么交换不能给增强样本引入足够的多样性;否则，如果太长，这个过程可能会给原句注入太多噪音。我们设置λ为比率，以便与原句子的长度不变。表2显示了一些具有不同长度约束的子树示例。我们将长度受限子树集合定义为:</p><p><span class="math display">\[S_{\lambda}(\mathbf{x}) \triangleq\left\{t \mid t \in S(\mathbf{x})\right., s.t. \left.\frac{|t|}{|\mathbf{x}|} \in\left[\lambda_{L}, \lambda_{U}\right]\right\}\]</span> 其中<span class="math inline">\(|·|\)</span>表示序列或子树的长度，对于两个句子<span class="math inline">\(x_i\)</span>和<span class="math inline">\(x_j\)</span>,我们随机采样两个子树<span class="math inline">\(t_{i}^{k} \in S_{\lambda}\left(\mathbf{x}_{i}\right)\)</span>和<span class="math inline">\(t_{j}^{l} \in S_{\lambda}\left(\mathbf{x}_{j}\right)\)</span>并且通过<span class="math inline">\(t_j^l\)</span>替换<span class="math inline">\(t_i^k\)</span>构建新的样本。例如 <span class="math display">\[\overline{\mathbf{x}} \triangleq[x_{i}^{1}, \ldots, x_{i}^{r_{k}-1}, \underbrace{x_{j}^{r_{l}}, \ldots, x_{j}^{s_{l}}}_{t_{j}^{l}}, x_{i}^{s_{k}+1}, \ldots x_{i}^{l}]\]</span> 其中<span class="math inline">\(t_{j}^{l}=\left[x_{j}^{r_{l} }, \ldots, x_{j}^{s_{l} }\right]\)</span>替换<span class="math inline">\(t_{i}^{k}=\)</span> <span class="math inline">\(\left[x_{i}^{r_{k}}, \ldots, x_{i}^{s_{k} }\right]\)</span>如上图1所示<em>a touching transcend love story</em> 替换<em>this poor film.</em></p><h4 id="treemix制作标签">TreeMix制作标签</h4><p>为扩充的样本<span class="math inline">\(\overline{x}\)</span>创建有效标签是一个具有挑战性的问题。类似于Mixup，我们使用原始的凸组合两个句子的标签作为扩充样本的标签。 <span class="math display">\[\overline{\mathbf{y}}=\frac{l_{i}-\left|t_{i}^{k}\right|}{l_{i}-\left|t_{i}^{k}\right|+\left|t_{j}^{l}\right|} \mathbf{y}_{i}+\frac{\left|t_{j}^{l}\right|}{l_{i}-\left|t_{i}^{k}\right|+\left|t_{j}^{l}\right|} \mathbf{y}_{j}\]</span> 其中<span class="math inline">\(l_i\)</span>为<span class="math inline">\(x_i\)</span>的长度，<span class="math inline">\(|t_i^k|\)</span>为子树的长度，在新的句子中,从<span class="math inline">\(x_i\)</span>中保留<span class="math inline">\(l_i-|t_i^k|\)</span>个words，从句子<span class="math inline">\(x_j\)</span>插入<span class="math inline">\(|t_j^l|\)</span>个words。</p><p><span class="math inline">\(\frac{l_{i}-\left|t_{i}^{k}\right|}{l_{i}-\left|t_{i}^{k}\right|+\left|t_{j}^{l}\right|}\)</span>是来自<span class="math inline">\(x_i\)</span>的words的分数，其可以决定<span class="math inline">\(y_i\)</span>的权重，然后，基于标签的变化与原始句子中的长度变化成比例的猜想来创建标签。附录提供样本。</p><figure><img src="https://cdn.jsdelivr.net/gh/zhoujiangfeng/images@main/20220614162151.png" alt="image-20220614162150953" /><figcaption aria-hidden="true">image-20220614162150953</figcaption></figure><h4 id="组合算法">组合算法</h4><p><img src="https://cdn.jsdelivr.net/gh/zhoujiangfeng/images@main/20220614163620.png" style="zoom:50%;" /></p><p>我们的主要算法如算法1所示。虽然TreeMix创建的句子<strong>并不都是流畅的甚至有效的新句子</strong>，但它们<strong>包含具有不同含义的子部分</strong>，这<strong>鼓励模型以组合的方式构建丰富的句子表示</strong>。需要注意的是，扩展后的标签是<strong>原始标签的凸组合</strong>，只有当模型学习到<strong>两个部分的表示在一起</strong>时，它们才能<strong>预测具有不同权重的两个标签</strong>。</p><h4 id="training-objective">Training Objective</h4><p>我们的模型是在原始样本和增强样本的组合上训练，以获得正则化和噪声注入之间的权衡。最终的培训目标是: <span class="math display">\[\begin{aligned}\mathcal{L}=&amp; \underset{(\mathbf{x}, \mathbf{y}) \sim D}{\mathbb{E}}\left[-\mathbf{y}^{\top} \log P_{\theta}(\mathbf{y} \mid \mathbf{x})\right]+\gamma \underset{(\overline{\mathbf{x} }, \overline{\mathbf{y} }) \sim D^{\prime} }{\mathbb{E}}\left[-\overline{\mathbf{y} }^{\top} \log P_{\theta}(\overline{\mathbf{y} } \mid \overline{\mathbf{x} })\right]\end{aligned}\]</span> <span class="math inline">\(\gamma\)</span>i是增强样本的权重</p><h2 id="experiment">Experiment</h2><ul><li><p>数据集</p><p><img src="https://cdn.jsdelivr.net/gh/zhoujiangfeng/images@main/20220614164635.png" alt="image-20220614164635190" style="zoom:50%;" /></p></li><li><p><strong>Baseline</strong></p><table><thead><tr class="header"><th>approaches</th><th>简介</th></tr></thead><tbody><tr class="odd"><td>BERT</td><td></td></tr><tr class="even"><td>EDA</td><td>由四个简单操作组成：同义词替换、随机插入、随机交换和随机删除。</td></tr><tr class="odd"><td>AEDA</td><td>在文本中随机插入标点符号的AEDA</td></tr><tr class="even"><td>Back Translation</td><td>将句子翻译成临时语言(EN-DE)，然后将先前翻译的文本翻译回源语言(DE-EN)</td></tr><tr class="odd"><td>GPT3Mix</td><td>设计提示并利用GPT3生成新的示例来训练模型。</td></tr><tr class="even"><td>SSMix</td><td>通过在给定类的所有示例前添加类标签来为条件BART。BARTword屏蔽了单个单词，而BARTspan屏蔽了连续的区块。</td></tr><tr class="odd"><td>EmbedMix</td><td></td></tr><tr class="even"><td>Tmix</td><td>首先对两个输入分别编码，然后在某一编码器层a处对两个嵌入进行线性插值，最终向前传递组合嵌入到其余层中。</td></tr></tbody></table></li><li><p><strong>结果</strong></p><p><img src="https://cdn.jsdelivr.net/gh/zhoujiangfeng/images@main/20220614170349.png" alt="image-20220614170349622" style="zoom:50%;" /></p></li></ul><figure><img src="https://cdn.jsdelivr.net/gh/zhoujiangfeng/images@main/20220614170416.png" alt="image-20220614170416075" /><figcaption aria-hidden="true">image-20220614170416075</figcaption></figure><figure><img src="https://cdn.jsdelivr.net/gh/zhoujiangfeng/images@main/20220614170443.png" alt="image-20220614170443863" /><figcaption aria-hidden="true">image-20220614170443863</figcaption></figure><h2 id="conclusion">Conclusion</h2><p>没找到代码，自己摸索了。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;publish&quot;&gt;Publish&lt;/h2&gt;
&lt;p&gt;NAACL2022&lt;/p&gt;
&lt;h2 id=&quot;title&quot;&gt;Title&lt;/h2&gt;
&lt;p&gt;TreeMix: Compositional Constituency-based Data Augmentation for </summary>
      
    
    
    
    <category term="DA" scheme="http://example.com/categories/DA/"/>
    
    
    <category term="NLP" scheme="http://example.com/tags/NLP/"/>
    
    <category term="DA" scheme="http://example.com/tags/DA/"/>
    
    <category term="MixUP" scheme="http://example.com/tags/MixUP/"/>
    
  </entry>
  
  <entry>
    <title>GAN原理总结及对比</title>
    <link href="http://example.com/2022/05/27/%E3%80%8CGAN%E3%80%8D%E5%90%84%E7%A7%8DGAN%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93%E5%AF%B9%E6%AF%94/"/>
    <id>http://example.com/2022/05/27/%E3%80%8CGAN%E3%80%8D%E5%90%84%E7%A7%8DGAN%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93%E5%AF%B9%E6%AF%94/</id>
    <published>2022-05-26T16:00:00.000Z</published>
    <updated>2022-06-22T03:46:12.369Z</updated>
    
    <content type="html"><![CDATA[<h2 id="原始gan">原始GAN</h2><p>GAN的主要灵感来源于博弈论中零和博弈的思想，应用到深度学习神经网络上来说，就是通过生成网络G（Generator）和判别网络D（Discriminator）不断博弈，进而使G学习到数据的分布，如果用到图片生成上，则训练完成后，G可以从一段随机数中生成逼真的图像。G， D的主要功能是：</p><p>● G是一个生成式的网络，它接收一个随机的噪声z（随机数），通过这个噪声生成图像</p><p>● D是一个判别网络，判别一张图片是不是“真实的”。它的输入参数是x，x代表一张图片，输出D（x）代表x为真实图片的概率，如果为1，就代表100%是真实的图片，而输出为0，就代表不可能是真实的图片</p><p>训练过程中，生成网络G的目标就是尽量生成真实的图片去欺骗判别网络D。而D的目标就是尽量辨别出G生成的假图像和真实的图像。这样，G和D构成了一个动态的“博弈过程”，最终的平衡点即纳什均衡点。</p><h3 id="gan的特点">GAN的特点：</h3><p>● 相比较传统的模型，他存在两个不同的网络，而不是单一的网络，并且训练方式采用的是对抗训练方式</p><p>● GAN中G的梯度更新信息来自判别器D，而不是来自数据样本</p><h3 id="gan-的优点">GAN 的优点：</h3><p>（以下部分摘自ian goodfellow 在Quora的问答）</p><p>● GAN是一种生成式模型，相比较其他生成模型（玻尔兹曼机和GSNs）只用到了反向传播,而不需要复杂的马尔科夫链</p><p>● 相比其他所有模型, GAN可以产生更加清晰，真实的样本</p><p>● GAN采用的是一种无监督的学习方式训练，可以被广泛用在无监督学习和半监督学习领域</p><p>● 相比于变分自编码器, GANs没有引入任何决定性偏置( deterministic bias),变分方法引入决定性偏置,因为他们优化对数似然的下界,而不是似然度本身,这看起来导致了VAEs生成的实例比GANs更模糊</p><p>● 相比VAE, GANs没有变分下界,如果鉴别器训练良好,那么生成器可以完美的学习到训练样本的分布.换句话说,GANs是渐进一致的,但是VAE是有偏差的</p><p>● GAN应用到一些场景上，比如图片风格迁移，超分辨率，图像补全，去噪，避免了损失函数设计的困难，不管三七二十一，只要有一个的基准，直接上判别器，剩下的就交给对抗训练了。 ### GAN的缺点：</p><p>● 训练GAN需要达到纳什均衡,有时候可以用梯度下降法做到,有时候做不到.我们还没有找到很好的达到纳什均衡的方法,所以训练GAN相比VAE或者PixelRNN是不稳定的,但我认为在实践中它还是比训练玻尔兹曼机稳定的多</p><p>● GAN不适合处理离散形式的数据，比如文本</p><p>● GAN存在训练不稳定、梯度消失、模式崩溃的问题（目前已解决）</p><p>模式崩溃(model collapse)原因</p><p>一般出现在GAN训练不稳定的时候，具体表现为生成出来的结果非常差，但是即使加长训练时间后也无法得到很好的改善。</p><p>具体原因可以解释如下：GAN采用的是对抗训练的方式，G的梯度更新来自D，所以G生成的好不好，得看D怎么说。具体就是G生成一个样本，交给D去评判，D会输出生成的假样本是真样本的概率（0-1），相当于告诉G生成的样本有多大的真实性，G就会根据这个反馈不断改善自己，提高D输出的概率值。但是如果某一次G生成的样本可能并不是很真实，但是D给出了正确的评价，或者是G生成的结果中一些特征得到了D的认可，这时候G就会认为我输出的正确的，那么接下来我就这样输出肯定D还会给出比较高的评价，实际上G生成的并不怎么样，但是他们两个就这样自我欺骗下去了，导致最终生成结果缺失一些信息，特征不全。</p><p>关于梯度消失的问题可以参考郑华滨的令人拍案叫绝的wassertein GAN，里面给出了详细的解释，不过多重复 <img src="https://cdn.jsdelivr.net/gh/zhoujiangfeng/images@main/20220527212456.png" alt="img" /></p><p>局部极小值点</p><p>原始GAN中判别器要最小化如下损失函数，尽可能把真实样本分为正例，生成样本分为负例：</p><p><span class="math inline">\(-\mathbb{E}_{x \sim P_{r}}[\log D(x)]-\mathbb{E}_{x \sim P_{g}}[\log (1-D(x))]\)</span>（公式1 )</p><p>其中<img src="https://zhihu.com/equation?tex=P_r" alt="P_r" />是真实样本分布，<img src="https://zhihu.com/equation?tex=P_g" alt="P_g" />是由生成器产生的样本分布。对于生成器，Goodfellow一开始提出来一个损失函数，后来又提出了一个改进的损失函数，分别是</p><p><span class="math inline">\(\mathbb{E}_{x \sim P_{g}}[\log (1-D(x))]\)</span>（公式2)</p><p><span class="math inline">\(\mathbb{E}_{x \sim P_{g}}[-\log D(x)]\)</span>（公式3）</p><h3 id="为什么gan不适合处理文本数据">为什么GAN不适合处理文本数据</h3><ol type="1"><li><p>文本数据相比较图片数据来说是离散的，因为对于文本来说，通常需要将一个词映射为一个高维的向量，最终预测的输出是一个one-hot向量，假设softmax的输出是（0.2， 0.3， 0.1，0.2，0.15，0.05）那么变为onehot是（0，1，0，0，0，0），如果softmax输出是（0.2， 0.25， 0.2， 0.1，0.15，0.1 ），one-hot仍然是（0， 1， 0， 0， 0， 0），所以对于生成器来说，G输出了不同的结果但是D给出了同样的判别结果，并不能将梯度更新信息很好的传递到G中去，所以D最终输出的判别没有意义。</p></li><li><p>另外就是GAN的损失函数是JS散度，JS散度不适合衡量不想交分布之间的距离。</p></li></ol><p>（WGAN虽然使用wassertein距离代替了JS散度，但是在生成文本上能力还是有限，GAN在生成文本上的应用有seq-GAN,和强化学习结合的产物）</p><h3 id="训练gan的一些技巧">训练GAN的一些技巧</h3><ol type="1"><li><p>输入规范化到（-1，1）之间，最后一层的激活函数使用tanh（BEGAN除外）</p></li><li><p>使用wassertein GAN的损失函数，</p></li><li><p>如果有标签数据的话，尽量使用标签，也有人提出使用反转标签效果很好，另外使用标签平滑，单边标签平滑或者双边标签平滑</p></li><li><p>使用mini-batch norm， 如果不用batch norm 可以使用instance norm 或者weight norm</p></li><li><p>避免使用RELU和pooling层，减少稀疏梯度的可能性，可以使用leakrelu激活函数</p></li><li><p>优化器尽量选择ADAM，学习率不要设置太大，初始1e-4可以参考，另外可以随着训练进行不断缩小学习率，</p></li><li><p>给D的网络层增加高斯噪声，相当于是一种正则</p></li></ol><hr /><p><a href="https://blog.csdn.net/Gavinmiaoc/article/details/79947877">来源</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;原始gan&quot;&gt;原始GAN&lt;/h2&gt;
&lt;p&gt;GAN的主要灵感来源于博弈论中零和博弈的思想，应用到深度学习神经网络上来说，就是通过生成网络G（Generator）和判别网络D（Discriminator）不断博弈，进而使G学习到数据的分布，如果用到图片生成上，则训练完</summary>
      
    
    
    
    <category term="GAN" scheme="http://example.com/categories/GAN/"/>
    
    
    <category term="GAN" scheme="http://example.com/tags/GAN/"/>
    
    <category term="NLP" scheme="http://example.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Counter-Contrastive 学习：训练Language GAN</title>
    <link href="http://example.com/2022/05/27/%E3%80%8CGAN%E3%80%8D2021EMNLP%EF%BC%9A%E5%AF%B9LGAN%E8%BF%9B%E8%A1%8CCounter-Contrastive%E5%AD%A6%E4%B9%A0/"/>
    <id>http://example.com/2022/05/27/%E3%80%8CGAN%E3%80%8D2021EMNLP%EF%BC%9A%E5%AF%B9LGAN%E8%BF%9B%E8%A1%8CCounter-Contrastive%E5%AD%A6%E4%B9%A0/</id>
    <published>2022-05-26T16:00:00.000Z</published>
    <updated>2022-06-22T03:45:48.471Z</updated>
    
    <content type="html"><![CDATA[<h2 id="publish">Publish</h2><p>2021EMNLP</p><h2 id="title">title</h2><p><a href="https://aclanthology.org/2021.findings-emnlp.415.pdf">Counter-Contrastive Learning for Language GANs</a></p><h2 id="摘要">摘要</h2><p>生成对抗网络(GANs)在图像合成方面取得了巨大的成功，但在生成自然语言方面存在一定的困难。挑战来自于鉴别器传递的uninformative learning signal。换句话说，<strong>糟糕的learning singnal限制了生成结构丰富、语义丰富的语言的学习能力</strong>。在本文中，我们提出在语言gan中采用反对比学习(CCL)方法来支持生成器的训练。与标准的GANs采用简单的二元分类器来区分样本的真假相比，我们采用了一种反对比学习信号，通过提高语言合成器的训练</p><ul><li>(1)把生成样本与真实样本 放在一起（以生成真实的数据）</li><li>(2)推开真实的样本（以阻碍鉴别的训练）从而防止鉴别器被过度训练。</li></ul><p>我们在合成基准和实际基准上评估了我们的方法，并与以前的GAN相比，在对抗序列生成方面产生了具有竞争力的性能。</p><h2 id="solution-problem">Solution problem</h2><p>CCL <span class="math display">\[\mathcal{L}_{i}=-\log \frac{e^{\operatorname{sim}\left(\mathbf{h}_{i}, \mathbf{h}_{i}^{-}\right) / \tau}}{\sum_{j=1}^{N}\left(e^{\operatorname{sim}\left(\mathbf{h}_{j}, \mathbf{h}_{j}^{-}\right) / \tau}+e^{\operatorname{sim}\left(\mathbf{h}_{j}, \mathbf{h}_{j}^{+}\right) / \tau}\right)}\]</span></p><h2 id="experiment">Experiment</h2><h2 id="conclusion">Conclusion</h2><p>没代码，不看了。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;publish&quot;&gt;Publish&lt;/h2&gt;
&lt;p&gt;2021EMNLP&lt;/p&gt;
&lt;h2 id=&quot;title&quot;&gt;title&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://aclanthology.org/2021.findings-emnlp.415.pdf&quot;&gt;C</summary>
      
    
    
    
    <category term="GAN" scheme="http://example.com/categories/GAN/"/>
    
    
    <category term="对比学习" scheme="http://example.com/tags/%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="GAN" scheme="http://example.com/tags/GAN/"/>
    
    <category term="NLP" scheme="http://example.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Text Smoothing：一种数据结合mix-up的数据增强方法</title>
    <link href="http://example.com/2022/05/26/%E3%80%8C%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E3%80%8D2022NAACL%EF%BC%9ATextSmoth/"/>
    <id>http://example.com/2022/05/26/%E3%80%8C%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E3%80%8D2022NAACL%EF%BC%9ATextSmoth/</id>
    <published>2022-05-25T16:00:00.000Z</published>
    <updated>2022-06-22T03:44:58.695Z</updated>
    
    <content type="html"><![CDATA[<h2 id="publish">Publish</h2><p>2022ACL</p><h2 id="title">title</h2><p>Text Smoothing: Enhance Various Data Augmentation Methods on Text Classification Tasks ## solution problem 进入神经网络之前，标记通常被转换为对应的One-hot表示，这是词汇表的离散分布。平滑表示是从预先训练的MLM中获得候选token的概率，它可以被视为对onr-hot表示的更多信息的替代。我们提出了一种有效的数据增强方法，称为文本平滑，通过将句子从其one-hot表示转换为可控的平滑表示。我们在低资源条件下对不同基准的文本平滑进行了评估。实验结果表明，文本平滑方法的性能明显优于各种主流数据增强方法。此外，文本平滑可以与这些数据增强方法相结合，以获得更好的性能。</p><p><img src="https://cdn.jsdelivr.net/gh/zhoujiangfeng/images@main/20220526171229.png" alt="image-20220526171221247" style="zoom:50%;" /></p><p><img src="https://cdn.jsdelivr.net/gh/zhoujiangfeng/images@main/20220526175945.png" alt="image-20220526175945737" style="zoom:50%;" /></p><p>文本平滑代码：Pytorch</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sentence = <span class="string">&quot;My favorite fruit is pear .&quot;</span></span><br><span class="line">lambd = <span class="number">0.1</span> <span class="comment"># interpolation hyperparameter</span></span><br><span class="line">mlm.train() <span class="comment"># enable dropout, dynamically mask</span></span><br><span class="line">tensor_input = tokenizer(sentence, return_tensors=<span class="string">&quot;pt&quot;</span>)</span><br><span class="line">onehot_repr = convert_to_onehot(**tensor_input)</span><br><span class="line">smoothed_repr = softmax(mlm(**tensor_input).logits[<span class="number">0</span>])</span><br><span class="line">interpolated_repr = lambd * onehot_repr + (<span class="number">1</span> - lambd) * smoothed_repr</span><br></pre></td></tr></table></figure><ol type="1"><li><p>使用BERT作为MLM，给定下游数据集命名为：<span class="math inline">\(D=\{t_i,p_i,s_i,l_i\}_{i=1}^{N}\)</span> ,N表示样本数量，<span class="math inline">\(t_i\)</span>表示文本one-hot 编码，<span class="math inline">\(p_i\)</span>表示<span class="math inline">\(t_i\)</span>位置编码，<span class="math inline">\(s_i\)</span>表示<span class="math inline">\(t_i\)</span>的段编码，<span class="math inline">\(l_i\)</span>表示实例标签。</p></li><li><p>将<span class="math inline">\(t_i,p_i,s_i\)</span>送入BERT</p></li><li><p>取回BERT中Transformer-encoder最后一层的输出表示为 <span class="math display">\[\overrightarrow{t_i}=BERT(t_i)\]</span> 其中<span class="math inline">\(\overrightarrow{t_i}\)</span>形状为[seq_len,emb_size]</p></li><li><p>然后乘以<span class="math inline">\(\overrightarrow{t_i}\)</span>乘以BERT中词嵌入矩阵<span class="math inline">\(W\)</span>,其形状为[vocab_size,embed_size] <span class="math display">\[MLM(t_i)=softmax(\overrightarrow(t_i)W^T)\]</span> 其中<span class="math inline">\(MLM(t_i)\)</span>中每一行是token词汇表中的概率分布，表示了预训练BERT学习到输入文本所在位置的包含上下文 的标记选项（信息）。</p></li><li></li><li><p>mixup定义为 <span class="math display">\[\tilde{x}=\lambda x_{i}+(1-\lambda) x_{j}\]</span></p><p><span class="math display">\[\tilde{y}=\lambda y_{i}+(1-\lambda) y_{j}\]</span></p><p>其中<span class="math inline">\((x_i,x_j),(y_i,y_j)\)</span>为从训练数据中随机抽出两个目标特征向量,<span class="math inline">\(\lambda\in[0,1]\)</span>在文本平滑中，One-hot表示和平滑表示来自相同的原始输入，标签相同，其内部插入操作不会改变标签，因此mixup操作可以简化为 <span class="math display">\[\widetilde{t_{i}}=\lambda \cdot t_{i}+(1-\lambda) \cdot \operatorname{MLM}\left(t_{i}\right)\]</span> 其中<span class="math inline">\(t_i\)</span>为one-hot表示，<span class="math inline">\(MLM(t_i)\)</span>为平滑表示，<span class="math inline">\(\widetilde{t_i}\)</span>为联合插入表示，<span class="math inline">\(\lambda\)</span>为用于控制插入的超参数。下游任务中我们使用联合表示代替one-hot变化表示作为输入。</p></li></ol><h2 id="experiment">Experiment</h2><ul><li><p>数据集</p><p><img src="https://cdn.jsdelivr.net/gh/zhoujiangfeng/images@main/20220526201719.png" alt="image-20220526201719743" style="zoom:50%;" /></p></li><li><p>Baseline</p><table><colgroup><col style="width: 27%" /><col style="width: 72%" /></colgroup><thead><tr class="header"><th>approaches</th><th>简介</th></tr></thead><tbody><tr class="odd"><td>EDA</td><td>由四个简单操作组成：同义词替换、随机插入、随机交换和随机删除。</td></tr><tr class="even"><td>Back Translation</td><td>将句子翻译成临时语言(EN-DE)，然后将先前翻译的文本翻译回源语言(DE-EN)</td></tr><tr class="odd"><td>CBERT</td><td>用预先训练的BERT mask一些标记并预测它们的上下文替换。</td></tr><tr class="even"><td>BERTexpand, BERTprepend</td><td>通过在给定类的所有示例中添加类标签来满足BERT条件。“expand”标签以 模拟 词汇表，而“prepend”则没有</td></tr><tr class="odd"><td>GPT2context</td><td>为预先训练的GPT模型提供提示，并持续生成，直到[EOS]token</td></tr><tr class="even"><td>BARTword, BARTspan</td><td>通过在给定类的所有示例前添加类标签来为条件BART。BARTword屏蔽了单个单词，而BARTspan屏蔽了连续的区块。</td></tr></tbody></table></li><li><p>结果</p><figure><img src="https://cdn.jsdelivr.net/gh/zhoujiangfeng/images@main/20220526202740.png" alt="image-20220526202740487" /><figcaption aria-hidden="true">image-20220526202740487</figcaption></figure></li></ul><h2 id="conclusion">Conclusion</h2><p>小数据，可控，目前优于其他，未来，结合其他DA=顶会。我也想发顶会啊。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;publish&quot;&gt;Publish&lt;/h2&gt;
&lt;p&gt;2022ACL&lt;/p&gt;
&lt;h2 id=&quot;title&quot;&gt;title&lt;/h2&gt;
&lt;p&gt;Text Smoothing: Enhance Various Data Augmentation Methods on Text </summary>
      
    
    
    
    <category term="DA" scheme="http://example.com/categories/DA/"/>
    
    
    <category term="NLP" scheme="http://example.com/tags/NLP/"/>
    
    <category term="MixUp" scheme="http://example.com/tags/MixUp/"/>
    
    <category term="DA" scheme="http://example.com/tags/DA/"/>
    
  </entry>
  
  <entry>
    <title>第二篇小论文的思考记录</title>
    <link href="http://example.com/2022/05/01/%E7%AC%AC%E4%BA%8C%E7%AF%87%E5%B0%8F%E8%AE%BA%E6%96%87%E7%9A%84%E6%80%9D%E8%80%83/"/>
    <id>http://example.com/2022/05/01/%E7%AC%AC%E4%BA%8C%E7%AF%87%E5%B0%8F%E8%AE%BA%E6%96%87%E7%9A%84%E6%80%9D%E8%80%83/</id>
    <published>2022-04-30T16:00:00.000Z</published>
    <updated>2022-06-22T03:43:20.801Z</updated>
    
    <content type="html"><![CDATA[<p>大论文的开题核心为”问句语义识别应用研究“。</p><p>第一篇小论文采用预训练模型进行数据增强以提升文本表征能力，主要研究集中在问句识别前期工作，并没有聚焦到语义识别。</p><p>对中文论文进行调研，大部分paper都将问句语义识别定义为一个文本分类问题，即问句分类。</p><p>所以第二篇小论文优先考虑<strong>文本分类问题</strong></p><p>近期又不可避免的 看了写GAN的文章，生成对抗网络，用于图像生成的确很不错。但是在nlp效果很一般，唯一给我的感觉就是这是一个很有学术价值的东西，适合写论文。</p><p><strong>seqGAN: sequence generative adversarial nets with policy gradient</strong></p><p>2017年发表 代码比较老 判别器用cnn，生成器rnn，改进点没想法，换成transformer？</p><p><strong>TT-gan:Text-to-Text Generative Adversarial Networks</strong> 2018年 模型不但可以生成真实文本，还能生成源文本释义或语义摘要。作者说是第一个 语义层面上生成自然语言的框框架。 <em>无代码</em> 没想法</p><p>重点来了“GAN-BERT”</p><p><strong>GAN-BERT: Generative Adversarial Learning for Robust Text Classification with a Bunch of Labeled Examples</strong> 2020acl。这篇文献我详细阅读了。 其判别器和生成器都是多层感知机，输入 noise（向量h） 进入生成器，输出<span class="math inline">\(h_{fake}\)</span> 然后在将真实数据 输入bert 获得向量 进行判别器。设置一个多分类任务进行训练，设置k+1个类，其中k为真实的类，k+1为生成器产生的类。过程如下：</p><ol type="1"><li>经过bert的向量设为<span class="math inline">\(h_{cls}\)</span> ，训练目标： 判别器真实样本将其归为k类，<span class="math inline">\(h_{fake}\)</span>归为k类</li><li>反向传播阶段， 无监督学习：无标签数据被错误归入k+1类时 优化 判别器的损失。有监督学习， 通同1要求 优化判别器损失。</li><li>训练结束，丢弃生成器，利用原始bert进行推理。（应该是bert+判别器）</li></ol><p>我的想法 能不能 用其他model替换mlp。（但是原文的一个创新就是 没有使用cnn），然后融入一些其他trick，提升分类效果。</p><p>更新于2022年05月01日</p><hr /><p>最近忙东忙西，不知道在干嘛。互联网+比赛，项目书、PPT、视频，小论文返修...</p><p>实验做了一点点。GAN，生成器，判别器，多层感知机 换成RNN。...</p><p>更新于2022年05月26日</p><hr />]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;大论文的开题核心为”问句语义识别应用研究“。&lt;/p&gt;
&lt;p&gt;第一篇小论文采用预训练模型进行数据增强以提升文本表征能力，主要研究集中在问句识别前期工作，并没有聚焦到语义识别。&lt;/p&gt;
&lt;p&gt;对中文论文进行调研，大部分paper都将问句语义识别定义为一个文本分类问题，即问句分类</summary>
      
    
    
    
    <category term="研究点" scheme="http://example.com/categories/%E7%A0%94%E7%A9%B6%E7%82%B9/"/>
    
    
    <category term="GAN" scheme="http://example.com/tags/GAN/"/>
    
    <category term="NLP" scheme="http://example.com/tags/NLP/"/>
    
    <category term="Mixup" scheme="http://example.com/tags/Mixup/"/>
    
  </entry>
  
  <entry>
    <title>SeqGAN：具有Policy梯度的序列生成对抗网络</title>
    <link href="http://example.com/2022/04/12/SeqGAN/"/>
    <id>http://example.com/2022/04/12/SeqGAN/</id>
    <published>2022-04-11T16:00:00.000Z</published>
    <updated>2022-06-22T03:43:30.065Z</updated>
    
    <content type="html"><![CDATA[<h2 id="publish">Publish</h2><p>AAAI-2017 ## title SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient ## solution problem 生成对抗网络（GAN）在生成real_value数据取得了巨大的成功,但是在用于生成离散数据具有局限性，主要原因在于：来自生成模型的离散输出使其难以从判别模型的梯度更新传递给生成模型。此外：判别模型只能评估完整的序列，而对于部分生成的序列，一旦生成完整序列，就需要去平衡当前和未来的评分。 ## Summary 1. 提出一种训练生成模型的新方法SeqGAN<br />2. 其数据生成器建模使用强化学习中的随机策略，其中RL奖励值来自GAN判别器对完整序列的评判。使用蒙特卡洛搜索传回中间状态。 ## Conclusion 在合成数据和现实任务上进行的大量实验表明，与强大的基线相比，有了显著的改进. # Other ## 蒙特卡洛搜索 学习资料 <a href="https://blog.csdn.net/luomin2523/article/details/118109154">1</a> <a href="https://blog.csdn.net/qq_16137569/article/details/83543641">2</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;publish&quot;&gt;Publish&lt;/h2&gt;
&lt;p&gt;AAAI-2017 ## title SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient ## solution problem 生成</summary>
      
    
    
    
    <category term="GAN" scheme="http://example.com/categories/GAN/"/>
    
    
    <category term="GAN" scheme="http://example.com/tags/GAN/"/>
    
    <category term="NLP" scheme="http://example.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>WhiteningBERT：降低向量维度且效果提高</title>
    <link href="http://example.com/2022/04/08/WhiteningBERT/"/>
    <id>http://example.com/2022/04/08/WhiteningBERT/</id>
    <published>2022-04-07T16:00:00.000Z</published>
    <updated>2022-06-22T03:43:39.567Z</updated>
    
    <content type="html"><![CDATA[<h1 id="背景">背景</h1><p>使用BERT等模型生成句向量，使用FAISS或ElasticSearch（ES）等引擎进行语义向量检索，是工业界常用的方法。然后在巨大的数据量时搜索时间成本巨大。</p><p>Whitening-BERT借鉴PCA方法对语义向量进行降维。有效提升语义相似度计算的效果。</p><p>首先介绍一下PCA降维降维方法。</p><h2 id="pca主成分分析">PCA（主成分分析）</h2><p><a href="https://www.bilibili.com/video/BV1E5411E71z?share_source=copy_web">用最直观的方式告诉你：什么是主成分分析PCA</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;背景&quot;&gt;背景&lt;/h1&gt;
&lt;p&gt;使用BERT等模型生成句向量，使用FAISS或ElasticSearch（ES）等引擎进行语义向量检索，是工业界常用的方法。然后在巨大的数据量时搜索时间成本巨大。&lt;/p&gt;
&lt;p&gt;Whitening-BERT借鉴PCA方法对语义向量进行</summary>
      
    
    
    
    <category term="BERT" scheme="http://example.com/categories/BERT/"/>
    
    
    <category term="BERT" scheme="http://example.com/tags/BERT/"/>
    
    <category term="语义相似度计算" scheme="http://example.com/tags/%E8%AF%AD%E4%B9%89%E7%9B%B8%E4%BC%BC%E5%BA%A6%E8%AE%A1%E7%AE%97/"/>
    
    <category term="降维" scheme="http://example.com/tags/%E9%99%8D%E7%BB%B4/"/>
    
  </entry>
  
  <entry>
    <title>SimCSE：一种对比学习数据增强方法</title>
    <link href="http://example.com/2022/04/01/simcse%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    <id>http://example.com/2022/04/01/simcse%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</id>
    <published>2022-03-31T16:00:00.000Z</published>
    <updated>2022-06-22T05:12:35.653Z</updated>
    
    <content type="html"><![CDATA[<h1 id="title">Title</h1><p>SimCSE: Simple Contrastive Learning of Sentence Embeddings</p><h2 id="time">Time</h2><p>2021.9</p><h2 id="publish">Publish</h2><p>Emnlp2021</p><h2 id="summary">Summary</h2><p><img src="https://cdn.jsdelivr.net/gh/zhoujiangfeng/images@main/20220323164848.png" alt="image-20220323164848858" style="zoom:25%;" /></p><ol type="1"><li>提出了目前在sentence embedding 领域表现最优的对比学习框架SimCSE</li><li>首先使用无监督方法，将输入sentence和其预测的sentence1作为对比目标，其只经过一个标准的dropout作为噪音，取得的很好效果，与以前的有监督效果相当。</li><li>其发现利用 dropout作为数据增强操作有很好的效果，且移除它会导致表示效果变差。</li><li>进而提出有监督方法。标注的自然语言推理数据集输入对比学习框架，使用“entailment”数据作为正样本，“contradiction”作为hard 负样本。</li><li>最后在STS任务评测，无监督、有监督simcse+bert_base 斯皮尔曼相关系数分别提升4.2%、2.2%</li><li>证明：对比学习 正则化了 预训练模型嵌入 各向异性空间变得更加统一，使用监督学习能够使得正样本更好的对齐。</li></ol><h2 id="methods">Method(s)</h2><h3 id="无监督simcse实现"><strong>无监督SimCSE实现</strong></h3><p><img src="https://cdn.jsdelivr.net/gh/zhoujiangfeng/images@main/20220323171412.png" alt="无监督SimCSE" style="zoom:25%;" /></p><ol type="1"><li>将句子输入 dropout作为noise</li><li>将同一句输入预训练模型编码器两次：应用标准的dropout两次。获得两个不同的嵌入作为“positive pairs”</li><li>同一个batch其他句子，组合作为“negatives”</li><li>最后模型从这些“negatives”中预测“positive”</li></ol><h3 id="train-objective">train objective</h3><p>同样的input到编码器两次，获得两个embedding<span class="math inline">\(z\)</span>,<span class="math inline">\(z^{\prime}\)</span></p><p><span class="math inline">\(\ell_{i}=-\log \frac{e^{\operatorname{sim}\left(\mathbf{h}_{i}^{z_{i} }, \mathbf{h}_{i}^{z_{i}^{\prime} }\right) / \tau} }{\sum_{j=1}^{N} e^{\operatorname{sim}\left(\mathbf{h}_{i}^{z_{i} }, \mathbf{h}_{j}^{z_{j}^{\prime} }\right) / \tau} }\)</span></p><p><span class="math inline">\(z\)</span>仅仅使用transformer标准的dropout mask</p><p><strong>结论：</strong>dropout对隐藏的表示起到了最小的“数据扩充”作用，而删除它则会导致表示崩溃。</p><h3 id="有监督simcse实现"><strong>有监督SimCSE实现</strong></h3><p><img src="https://cdn.jsdelivr.net/gh/zhoujiangfeng/images@main/20220324113118.png" alt="image-20220324113118463" style="zoom:40%;" /></p><ul><li>使用自然语言推理数据集，样例如上</li></ul><p>1.将原有三分类任务（蕴含，中立，相反）降低为（蕴含，相反），其中“中立”被视为正样本。</p><p>2.添加对应的相反 语句对视为 hard negative 进一步提升表现。NLI数据集效果尤其好。</p><h3 id="train-objective-1">train objective</h3><p>将<span class="math inline">\(x，x_i^+\)</span>扩展为<span class="math inline">\(x，x_i^+，x_i^-\)</span>,以<span class="math inline">\(x\)</span>为基础，<span class="math inline">\(x_i^+\)</span>和<span class="math inline">\(x_i^-\)</span>为蕴含和相反 假设</p><p>其训练目标为</p><p>​ <span class="math inline">\(-\log \frac{e^{\operatorname{sim}\left(\mathbf{h}_{i}, \mathbf{h}_{i}^{+}\right) / \tau} }{\sum_{j=1}^{N}\left(e^{\operatorname{sim}\left(\mathbf{h}_{i}, \mathbf{h}_{j}^{+}\right) / \tau}+e^{\operatorname{sim}\left(\mathbf{h}_{i}, \mathbf{h}_{j}^{-}\right) / \tau}\right)}\)</span></p><h2 id="evaluation">Evaluation</h2><p>对比学习分析工具：</p><ol type="1"><li><p>alignment</p><p>测试语义相关的正样本对齐程度</p><p><span class="math inline">\(\ell_{\text {align } } \triangleq \underset{\left(x, x^{+}\right) \sim p_{\text {pos } } }{\mathbb{E} }\left\|f(x)-f\left(x^{+}\right)\right\|^{2}\)</span></p><p>其中<span class="math inline">\(p_{pos}\)</span>表示 语句对对应的分布式的句向量，假设其中表示均已正则化</p></li><li><p>uniformity</p><p>利用整个表示空间的均匀性来衡量学习嵌入的质量</p></li></ol><p>​ <span class="math inline">\(\ell_{\text {uniform } } \triangleq \log \underset{x, y \stackrel{\mathbb{i} . i . d .}{\sim} p_{\text {data } } }{ } e^{-2\|f(x)-f(y)\|^{2} }\)</span></p><p>这两个指标与对比学习的目标是一致的:正面实例应该保持接近，而随机实例的嵌入应该分散在超球体上。<img src="https://cdn.jsdelivr.net/gh/zhoujiangfeng/images@main/20220324155942.png" alt="image-20220324155942504" style="zoom: 33%;" /><img src="/Users/zhoujiangfeng/Library/Application%20Support/typora-user-images/image-20220324170115756.png" alt="image-20220324170115756" style="zoom:50%;" /></p><p>其数字越小越好</p><h2 id="conclusion">Conclusion</h2><h6 id="section"><img src="https://cdn.jsdelivr.net/gh/zhoujiangfeng/images@main/20220324164727.png" alt="image-20220324164727102" style="zoom:50%;" /></h6><p><img src="https://cdn.jsdelivr.net/gh/zhoujiangfeng/images@main/20220324164933.png" alt="image-20220324164933309" style="zoom: 50%;" /></p><h2 id="notes">Notes</h2><h4 id="对比学习"><strong>对比学习</strong>：</h4><p><em>对比学习的目的是通过把语义上相近的邻居拉到一起，把非邻居推开来学习有效的表征</em></p><p>假定语句集<span class="math inline">\(\mathcal{D}=\left\{\left(x_{i}, x_{i}^{+}\right)\right\}_{i=1}^{m}\)</span>，其<span class="math inline">\(x_{i}\)</span>和<span class="math inline">\(x_{i}^{+}\)</span>语义相关</p><p>采用交叉熵损失</p><p>其<span class="math inline">\(h_{i}\)</span>和<span class="math inline">\(h_{i}^{+}\)</span>表示<span class="math inline">\(x_{i}\)</span>和<span class="math inline">\(x_{i}^{+}\)</span></p><p>对于（<span class="math inline">\(x_{i}\)</span>，<span class="math inline">\(x_{i}^{+}\)</span>）的Npair mini-batch 的训练目标为</p><p>$<em>{i}=- {</em>{j=1}^{N} e<sup>{(<em>{i}, </em>{j}</sup>{+}) / } } $</p><p>其中<span class="math inline">\(\tau\)</span>为超参数，<span class="math inline">\(\operatorname{sim}\left(\mathbf{h}_{i}, \mathbf{h}_{i}^{+}\right)\)</span>表示cos相似度<span class="math inline">\(\frac{\mathbf{h}_{1}^{\top} \mathbf{h}_{2} }{\left\|\mathbf{h}_{1}\right\| \cdot\left\|\mathbf{h}_{2}\right\|}\)</span></p><p><strong>使用步骤</strong></p><p>首先利用PLM编码输入sentence</p><p>然后利用上述对比学习目标fine-tune所有参数</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;title&quot;&gt;Title&lt;/h1&gt;
&lt;p&gt;SimCSE: Simple Contrastive Learning of Sentence Embeddings&lt;/p&gt;
&lt;h2 id=&quot;time&quot;&gt;Time&lt;/h2&gt;
&lt;p&gt;2021.9&lt;/p&gt;
&lt;h2 id=&quot;pu</summary>
      
    
    
    
    <category term="数据增强" scheme="http://example.com/categories/%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA/"/>
    
    
    <category term="对比学习" scheme="http://example.com/tags/%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="数据增强" scheme="http://example.com/tags/%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA/"/>
    
  </entry>
  
</feed>
